{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202b52a9-a733-469f-b7b2-5d0ec82ece2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ìƒíƒœê°€ì¹˜í•¨ìˆ˜ VÏ€(s):\n",
      "  S: 2.4157\n",
      "  R1: 2.7260\n",
      "  R2: 3.5000\n",
      "  R3: 3.0000\n",
      "  F: 0.0000\n",
      "\n",
      "âœ… í–‰ë™ê°€ì¹˜í•¨ìˆ˜ QÏ€(s,a):\n",
      "  (S, A1): 1.8606\n",
      "  (S, A2): 3.2500\n",
      "  (R1, A1): 2.5000\n",
      "  (R1, A2): 3.2500\n",
      "  (R2, A1): 3.5000\n",
      "  (R3, A1): 3.0000\n",
      "\n",
      "ğŸŒŸ ìµœì ì •ì±… Ï€*(s):\n",
      "  ìƒíƒœ S â†’ ìµœì  í–‰ë™: A2\n",
      "  ìƒíƒœ R1 â†’ ìµœì  í–‰ë™: A2\n",
      "  ìƒíƒœ R2 â†’ ìµœì  í–‰ë™: A1\n",
      "  ìƒíƒœ R3 â†’ ìµœì  í–‰ë™: A1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# (1) ìƒíƒœì™€ í–‰ë™ ì •ì˜\n",
    "states = [\"S\", \"R1\", \"R2\", \"R3\", \"F\"]\n",
    "actions = [\"A1\", \"A2\"]\n",
    "gamma = 0.5  # ê°ê°€ìœ¨\n",
    "\n",
    "# (2) ì •ì±… Ï€(s, a): ìƒíƒœì—ì„œ í–‰ë™ì„ ì„ íƒí•  í™•ë¥ , ì •ì±…ì´ ê³ ì •ë¨\n",
    "policy = {\n",
    "    \"S\": {\"A1\": 0.6, \"A2\": 0.4},\n",
    "    \"R1\": {\"A1\": 0.7, \"A2\": 0.3},\n",
    "    \"R2\": {\"A1\": 1.0},\n",
    "    \"R3\": {\"A1\": 1.0}\n",
    "}\n",
    "\n",
    "# (3) ìƒíƒœ ì „ì´ í™•ë¥  P(s, a, s')\n",
    "transition_probs = {\n",
    "    (\"S\", \"A1\"): \"R1\",\n",
    "    (\"S\", \"A2\"): \"R2\",\n",
    "    (\"R1\", \"A1\"): \"R3\",\n",
    "    (\"R1\", \"A2\"): \"R2\",\n",
    "    (\"R2\", \"A1\"): \"R3\",\n",
    "    (\"R3\", \"A1\"): \"F\"\n",
    "}\n",
    "\n",
    "# (4) ë³´ìƒ í•¨ìˆ˜ R(s, a)\n",
    "rewards = {\n",
    "    (\"S\", \"A1\"): 0.5,\n",
    "    (\"S\", \"A2\"): 1.5,\n",
    "    (\"R1\", \"A1\"): 1.0,\n",
    "    (\"R1\", \"A2\"): 1.5,\n",
    "    (\"R2\", \"A1\"): 2.0,\n",
    "    (\"R3\", \"A1\"): 3.0\n",
    "}\n",
    "\n",
    "# (5) ì—í”¼ì†Œë“œ ì‹œë®¬ë ˆì´ì…˜ í•¨ìˆ˜ (ì •ì±… ê¸°ë°˜ ê²½ë¡œ)\n",
    "def simulate_episode(start_state=\"S\"):\n",
    "    state = start_state\n",
    "    total_return = 0\n",
    "    discount = 1.0\n",
    "\n",
    "    #(5)-1 ì—í”¼ì†Œë“œ ë°˜ë³µ ì‹¤í–‰\n",
    "    while state != \"F\":\n",
    "        \n",
    "        # í˜„ì¬ ìƒíƒœì—ì„œ ì •ì±…ì— ë”°ë¼ í–‰ë™ ì„ íƒ\n",
    "        action_probs = policy[state]                     #í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ë“¤ê³¼ ê·¸ì— ëŒ€í•œ í™•ë¥ (policy)ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
    "        actions_list = list(action_probs.keys())         #í–‰ë™ ì´ë¦„ë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•œë‹¤. (ì˜ˆ) [\"A1\", \"A2\"]\n",
    "        probs = list(action_probs.values())              #í–‰ë™ ì„ íƒ í™•ë¥ ë§Œ ë”°ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•œë‹¤. (ì˜ˆ) [0.6, 0.4]\n",
    "        action = np.random.choice(actions_list, p=probs) #ì„ íƒì€ í™•ë¥ ì ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤. ì˜ˆ: 60% í™•ë¥ ë¡œ \"A1\", 40% í™•ë¥ ë¡œ \"A2\" ì„ íƒ\n",
    "\n",
    "        reward = rewards.get((state, action), 0)         #í˜„ì¬ ìƒíƒœì™€ ì„ íƒí•œ í–‰ë™ì— ëŒ€í•œ ë³´ìƒì„ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
    "        total_return += discount * reward                #ê°ê°€ìœ¨ì„ ì ìš©í•œ í˜„ì¬ ë³´ìƒì„ ì´ ë³´ìƒì— ë”í•œë‹¤.\n",
    "\n",
    "        next_state = transition_probs.get((state, action), \"F\") #í˜„ì¬ ìƒíƒœì™€ í–‰ë™ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ìƒíƒœë¥¼ ì¡°íšŒí•œë‹¤.\n",
    "        state = next_state                                      #í˜„ì¬ ìƒíƒœë¥¼ ë‹¤ìŒ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸. ì—ì´ì „íŠ¸ëŠ” í•œ ë‹¨ê³„ ì•ìœ¼ë¡œ ì§„í–‰\n",
    "        discount *= gamma                                       #ê°ê°€ìœ¨ì„ ëˆ„ì  ì ìš©í•œë‹¤.\n",
    "\n",
    "    return total_return\n",
    "\n",
    "n_episodes = 10000\n",
    "state_values = {}\n",
    "q_values = {}\n",
    "\n",
    "# (6) ìƒíƒœê°€ì¹˜í•¨ìˆ˜ ê³„ì‚°\n",
    "for state in states:\n",
    "    episode_returns = []\n",
    "    for _ in range(n_episodes):\n",
    "        result = simulate_episode(start_state=state)\n",
    "        episode_returns.append(result)\n",
    "    state_values[state] = np.mean(episode_returns) #ëª¨ë“  ìƒíƒœì— ëŒ€í•´ ì—í”¼ì†Œë“œë¥¼ 1ë§Œ ë²ˆ ì‹œë®¬ë ˆì´ì…˜, í‰ê·  ì´ ë³´ìƒ ğ‘‰ğœ‹(ğ‘ ) ì¶”ì •\n",
    "\n",
    "# (7) í–‰ë™ê°€ì¹˜í•¨ìˆ˜ ê³„ì‚°: ìƒíƒœ-í–‰ë™ ìŒ ê°ê°ì— ëŒ€í•´ Qê°’ì„ ì¶”ì •\n",
    "for state in policy:                #ì •ì±…ì´ ì •ì˜ëœ ëª¨ë“  ìƒíƒœ(state)ë¥¼ í•˜ë‚˜ì”© êº¼ë‚¸ë‹¤. ì˜ˆ: \"S\", \"R1\", ...\n",
    "    for action in policy[state]:    #í•´ë‹¹ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™(action)ì„ ìˆœíšŒí•œë‹¤. ì˜ˆ: \"A1\", \"A2\"\n",
    "        returns = []                #í˜„ì¬ ìƒíƒœ-í–‰ë™ ìŒì— ëŒ€í•´ ì—í”¼ì†Œë“œë¥¼ ì—¬ëŸ¬ ë²ˆ ëŒë ¤ ì–»ì€ ë°˜í™˜ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "        for _ in range(n_episodes):\n",
    "            temp_state = state\n",
    "            total_return = 0\n",
    "            discount = 1.0\n",
    "\n",
    "            #(7-1) ì²« í–‰ë™ì„ ê°•ì œë¡œ ì„ íƒ\n",
    "            reward = rewards.get((temp_state, action), 0) #ì²« ë²ˆì§¸ í–‰ë™ì€ í™•ë¥ ì´ ì•„ë‹ˆë¼ ë°˜ë“œì‹œ ì§€ì •ëœ a ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨\n",
    "            total_return += discount * reward             #ì²˜ìŒì€ discout=1, ì¦‰ì‹œë³´ìƒ ê³„ì‚°\n",
    "            next_state = transition_probs.get((temp_state, action), \"F\")\n",
    "            temp_state = next_state\n",
    "            discount *= gamma\n",
    "\n",
    "            #(7-2) ì´í›„ë¶€í„°ëŠ” ì •ì±…ì— ë”°ë¼ í–‰ë™ -> ìƒíƒœê°€ì¹˜ í•¨ìˆ˜ì™€ ë™ì¼í•œ ìˆ˜ì‹\n",
    "            while temp_state != \"F\":\n",
    "                action_probs = policy[temp_state]\n",
    "                actions_list = list(action_probs.keys())\n",
    "                probs = list(action_probs.values())\n",
    "                next_action = np.random.choice(actions_list, p=probs) #--> í–‰ë™ì„ ì •ì±…ì— ì˜í•´ì„œ ê²°ì •\n",
    "\n",
    "                reward = rewards.get((temp_state, next_action), 0)\n",
    "                total_return += discount * reward\n",
    "                temp_state = transition_probs.get((temp_state, next_action), \"F\")\n",
    "                discount *= gamma\n",
    "\n",
    "            returns.append(total_return)\n",
    "            \n",
    "        # í‰ê·  ë³´ìƒì„ í–‰ë™ê°€ì¹˜ë¡œ ì €ì¥, **ê° ìƒíƒœ/í–‰ë™ë³„ë¡œ ê°œë³„ ì ìœ¼ë¡œ ì €ì¥**\n",
    "        q_values[(state, action)] = np.mean(returns)\n",
    "\n",
    "# (8) ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\nâœ… ìƒíƒœê°€ì¹˜í•¨ìˆ˜ VÏ€(s):\")\n",
    "for s, v in state_values.items():\n",
    "    print(f\"  {s}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… í–‰ë™ê°€ì¹˜í•¨ìˆ˜ QÏ€(s,a):\")\n",
    "for (s, a), q in q_values.items():\n",
    "    print(f\"  ({s}, {a}): {q:.4f}\")\n",
    "\n",
    "# (9) ìµœì ì •ì±… ê³„ì‚° (ê°€ì¥ Qê°’ì´ ë†’ì€ í–‰ë™ë§Œ ì„ íƒ)\n",
    "optimal_policy = {}\n",
    "for state in policy:\n",
    "    best_action = None\n",
    "    best_q_value = float(\"-inf\")\n",
    "\n",
    "    # ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì— ëŒ€í•´ Qê°’ì„ ë¹„êµí•´ì„œ ê°€ì¥ í° ê²ƒ ì„ íƒ\n",
    "    for action in policy[state]:\n",
    "        q = q_values.get((state, action), float(\"-inf\"))\n",
    "        if q > best_q_value:\n",
    "            best_q_value = q\n",
    "            best_action = action\n",
    "\n",
    "    # ìµœì ì •ì±…: ê°€ì¥ ì¢‹ì€ í–‰ë™ë§Œ í™•ë¥  1ë¡œ ì„¤ì •\n",
    "    action_probabilities = {}\n",
    "    for action in policy[state]:\n",
    "        if action == best_action:\n",
    "            action_probabilities[action] = 1.0 #ëª¨ë“  ìƒíƒœì— ëŒ€í•´ í™•ë¥  1.0ë¡œ ë‹¨ í•˜ë‚˜ì˜ í–‰ë™ë§Œ ì„ íƒí•˜ëŠ” ê²°ì •ë¡ ì  ì •ì±…ì´ ì™„ì„±\n",
    "        else:\n",
    "            action_probabilities[action] = 0.0\n",
    "    '''\n",
    "    {\n",
    "        \"S\": {\"A1\": 0.0, \"A2\": 1.0},\n",
    "        \"R1\": {\"A1\": 1.0, \"A2\": 0.0},\n",
    "        ...\n",
    "    }\n",
    "    '''\n",
    "    optimal_policy[state] = action_probabilities\n",
    "\n",
    "# (10) ìµœì ì •ì±… ì¶œë ¥\n",
    "print(\"\\nğŸŒŸ ìµœì ì •ì±… Ï€*(s):\")\n",
    "for state in optimal_policy:\n",
    "    for action, prob in optimal_policy[state].items():\n",
    "        if prob == 1.0:\n",
    "            print(f\"  ìƒíƒœ {state} â†’ ìµœì  í–‰ë™: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba11d8-56e7-4f88-a675-407b07805a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
