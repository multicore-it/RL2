{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc2b883-9071-485d-aa41-6089a2c41315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./ppo_nlp_log/PPO_2\n",
      "Eval num_timesteps=1000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | 0.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | 0.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.265    |\n",
      "| time/              |          |\n",
      "|    fps             | 1888     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05500397 |\n",
      "|    clip_fraction        | 0.78       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.56      |\n",
      "|    explained_variance   | -0.139     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0724    |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.139     |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | 1        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.357    |\n",
      "| time/              |          |\n",
      "|    fps             | 1380     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10558325 |\n",
      "|    clip_fraction        | 0.937      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.31      |\n",
      "|    explained_variance   | 0.0112     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.082     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.173     |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | 1        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.633    |\n",
      "| time/              |          |\n",
      "|    fps             | 1280     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12900522 |\n",
      "|    clip_fraction        | 0.96       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.841     |\n",
      "|    explained_variance   | -0.00424   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0802    |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.187     |\n",
      "|    value_loss           | 0.219      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | 1        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.759    |\n",
      "| time/              |          |\n",
      "|    fps             | 1230     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22298223 |\n",
      "|    clip_fraction        | 0.902      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.258     |\n",
      "|    explained_variance   | 0.0048     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0826    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.155     |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | 1        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.901    |\n",
      "| time/              |          |\n",
      "|    fps             | 1204     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "\n",
      "[모델 테스트]\n",
      "Q: 강화학습이란?\n",
      "A: 강화학습은 보상을 통해 학습하는 방법입니다.\n",
      "------------------------------\n",
      "Q: 불면증 해결 방법은?\n",
      "A: 네, 불면증은 괴로운 문제일 수 있습니다.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "#자연어 처리 피드백 환경 정의\n",
    "class NLPFeedbackEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        #(1) 클래스 초기화\n",
    "        super(NLPFeedbackEnv, self).__init__()\n",
    "        \n",
    "        #(2) 상태 공간 정의, 질문 ID: 0, 1\n",
    "        self.state_space = 2\n",
    "        self.observation_space = spaces.Discrete(self.state_space)\n",
    "        \n",
    "        #(3) 행동 공간 정의, 에이전트는 5개의 응답 중 하나를 선택\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        #(4) 현재 질문 상태 초기화\n",
    "        self.current_prompt = 0\n",
    "\n",
    "        #(5) 보상 테이블 정의\n",
    "        self.reward_table = {\n",
    "            (0, 0): 1.0,  # 좋은 설명\n",
    "            (0, 1): 0.3,\n",
    "            (1, 3): 1.0,  # 공감 있는 응답\n",
    "            (1, 4): 0.2\n",
    "        }\n",
    "\n",
    "        #(6) 질문 텍스트 매핑\n",
    "        self.prompts = {\n",
    "            0: \"강화학습이란?\",\n",
    "            1: \"불면증 해결 방법은?\"\n",
    "        }\n",
    "\n",
    "        #(7) 응답 텍스트 매핑\n",
    "        self.responses = {\n",
    "            0: \"강화학습은 보상을 통해 학습하는 방법입니다.\",\n",
    "            1: \"AI의 한 분야입니다.\",\n",
    "            2: \"잘 모르겠습니다.\",\n",
    "            3: \"네, 불면증은 괴로운 문제일 수 있습니다.\",\n",
    "            4: \"수면제를 드세요.\"\n",
    "        }\n",
    "\n",
    "    #환경상태 초기화\n",
    "    def reset(self):\n",
    "        #(1) 질문(prompt)을 무작위로 선택\n",
    "        self.current_prompt = np.random.choice([0, 1])\n",
    "\n",
    "        #(2) 선택된 질문 ID를 넘파이 배열 형태로 반환\n",
    "        return np.array([self.current_prompt], dtype=np.int32)\n",
    "\n",
    "    #강화학습 환경의 핵심 루프인 상태 → 행동 → 보상 → 다음 상태의 흐름을 담당\n",
    "    def step(self, action):\n",
    "        #(1) 에이전트의 행동에 따른 보상 계산\n",
    "        reward = self.reward_table.get((self.current_prompt, action), 0.0)\n",
    "\n",
    "        #(2) 에피소드 종료 처리\n",
    "        done = True\n",
    "\n",
    "        #(3) 다음 상태 초기화 (새로운 질문 선택)\n",
    "        obs = self.reset()\n",
    "\n",
    "        #(4) 디버깅 및 모니터링 정보 제공\n",
    "        info = {\n",
    "            \"prompt\": self.prompts[self.current_prompt],\n",
    "            \"response\": self.responses[action],\n",
    "            \"reward\": reward\n",
    "        }\n",
    "\n",
    "        #(5) 환경 반환값 구성\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# 환경 구성\n",
    "#(1) 학습용 환경 인스턴스 생성\n",
    "env = NLPFeedbackEnv()\n",
    "\n",
    "#(2) 평가용 환경 인스턴스 생성\n",
    "eval_env = NLPFeedbackEnv()\n",
    "\n",
    "#(3) 평가 콜백 설정\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs_nlp/best_model\",\n",
    "    log_path=\"./logs_nlp/\",\n",
    "    eval_freq=1000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# 모델 정의\n",
    "#(1) 모델 정의\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./ppo_nlp_log/\")\n",
    "\n",
    "#(2) 학습 시작\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# 테스트\n",
    "print(\"\\n[모델 테스트]\")\n",
    "#(1) 질문 루프 시작\n",
    "for prompt_id in [0, 1]:\n",
    "\n",
    "    #(2) 질문 ID를 상태 벡터로 변환\n",
    "    obs = np.array([prompt_id])\n",
    "\n",
    "    #(2) 정책에 따라 행동 예측\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    #(3) 정수 인덱스로 변환\n",
    "    action = int(action)\n",
    "\n",
    "    #(4) 질문 및 모델 응답 출력\n",
    "    print(f\"Q: {env.prompts[prompt_id]}\")\n",
    "    print(f\"A: {env.responses[action]}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d3c34-cec6-45a5-b479-425ffa214280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
