{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d91b889-ee19-482a-b788-5a9348a13a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정책을 따른 기대 반환값(에피소드 평균, 상태가치함수): 2.4168\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#(1)상태와 행동, 감가율 정의\n",
    "states = [\"S\", \"R1\", \"R2\", \"R3\", \"F\"]\n",
    "actions = [\"A1\", \"A2\"]\n",
    "gamma = 0.5\n",
    "\n",
    "#(2)정책 π(s, a): 상태 s에서 행동 a를 선택할 확률\n",
    "policy = {\n",
    "    \"S\": {\"A1\": 0.6, \"A2\": 0.4},\n",
    "    \"R1\": {\"A1\": 0.7, \"A2\": 0.3},\n",
    "    \"R2\": {\"A1\": 1.0},\n",
    "    \"R3\": {\"A1\": 1.0}\n",
    "}\n",
    "\n",
    "#(3) 상태 전이 확률 P(s, a, s'), 결정적 전이\n",
    "transition_probs = {\n",
    "    (\"S\", \"A1\"): \"R1\",\n",
    "    (\"S\", \"A2\"): \"R2\",\n",
    "    (\"R1\", \"A1\"): \"R3\",\n",
    "    (\"R1\", \"A2\"): \"R2\",\n",
    "    (\"R2\", \"A1\"): \"R3\",\n",
    "    (\"R3\", \"A1\"): \"F\"\n",
    "}\n",
    "\n",
    "#(4) 보상 함수 R(s, a)\n",
    "rewards = {\n",
    "    (\"S\", \"A1\"): 0.5,\n",
    "    (\"S\", \"A2\"): 1.5,\n",
    "    (\"R1\", \"A1\"): 1.0,\n",
    "    (\"R1\", \"A2\"): 1.5,\n",
    "    (\"R2\", \"A1\"): 2.0,\n",
    "    (\"R3\", \"A1\"): 3.0\n",
    "}\n",
    "\n",
    "#(5) 경로 시뮬레이션 기반 기대 반환값 계산\n",
    "def simulate_episode(start_state=\"S\"):\n",
    "    state = start_state\n",
    "    total_return = 0\n",
    "    discount = 1.0 #감가율(γ)을 누적 곱해주는 변수, 이후 단계마다 γ만큼 곱해지며 미래 보상에 대한 가치를 점점 줄인다.\n",
    "\n",
    "    #(6) 에피소드 반복 실행\n",
    "    while state != \"F\":\n",
    "        #(6)-1 현재 상태에서 정책에 따라 행동 선택\n",
    "        action_probs = policy[state]                     #현재 상태에서 가능한 행동들과 그에 대한 확률(policy)을 불러온다.\n",
    "        actions_list = list(action_probs.keys())         #행동 이름들만 리스트로 추출한다. (예) [\"A1\", \"A2\"]\n",
    "        probs = list(action_probs.values())              #행동 선택 확률만 따로 리스트로 추출한다. (예) [0.6, 0.4]\n",
    "        action = np.random.choice(actions_list, p=probs) #선택은 확률적으로 이루어진다. 예: 60% 확률로 \"A1\", 40% 확률로 \"A2\" 선택\n",
    "\n",
    "        #(6)-2 보상 받기\n",
    "        reward = rewards.get((state, action), 0)  #현재 상태와 선택한 행동에 대한 보상을 불러온다.\n",
    "        total_return += discount * reward         #감가율을 적용한 현재 보상을 총 보상에 더한다.\n",
    "\n",
    "        #(6)-3 다음 상태로 이동\n",
    "        next_state = transition_probs.get((state, action), \"F\") #현재 상태와 행동을 기반으로 다음 상태 조회.\n",
    "        state = next_state                                      #현재 상태를 다음 상태로 업데이트. 에이전트는 한 단계 앞으로 진행.\n",
    "        discount *= gamma                                       #감가율을 누적 적용한다.\n",
    "\n",
    "    return total_return\n",
    "\n",
    "#(7) 여러 번 실행하여 평균 반환값 추정\n",
    "n_episodes = 10000\n",
    "returns = [] \n",
    "\n",
    "for i in range(n_episodes):\n",
    "    episode_return = simulate_episode()\n",
    "    returns.append(episode_return) # 에피소드별 반환값 저장\n",
    "expected_return = np.mean(returns)\n",
    "\n",
    "print(f\"정책을 따른 기대 반환값(에피소드 평균, 상태가치함수): {expected_return:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd7cc6-6538-457d-9493-3f8427860505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
