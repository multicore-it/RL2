{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98597d6-eece-4b6a-acf9-48df3faa7fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 상태 가치 테이블:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "최종 정책(각 상태에서의 행동 확률):\n",
      "상태 (0,0): [0. 0. 0. 0.]\n",
      "상태 (0,1): [0. 0. 1. 0.]\n",
      "상태 (0,2): [0. 0. 1. 0.]\n",
      "상태 (0,3): [0.  0.5 0.5 0. ]\n",
      "상태 (1,0): [1. 0. 0. 0.]\n",
      "상태 (1,1): [0.5 0.  0.5 0. ]\n",
      "상태 (1,2): [0.25 0.25 0.25 0.25]\n",
      "상태 (1,3): [0. 1. 0. 0.]\n",
      "상태 (2,0): [1. 0. 0. 0.]\n",
      "상태 (2,1): [0.25 0.25 0.25 0.25]\n",
      "상태 (2,2): [0.  0.5 0.  0.5]\n",
      "상태 (2,3): [0. 1. 0. 0.]\n",
      "상태 (3,0): [0.5 0.  0.  0.5]\n",
      "상태 (3,1): [0. 0. 0. 1.]\n",
      "상태 (3,2): [0. 0. 0. 1.]\n",
      "상태 (3,3): [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# (1) 그리드월드 환경 설정 (4x4)\n",
    "grid_size = 4\n",
    "gamma = 1.0  # 감가율\n",
    "\n",
    "# (2) 상태 가치 초기화\n",
    "value_table = np.zeros((grid_size, grid_size))\n",
    "\n",
    "# (3) 종료 상태(목적지)\n",
    "terminal_states = [(0, 0), (grid_size - 1, grid_size - 1)]\n",
    "\n",
    "# (4) 가능한 행동: (행,열)상, 하, 좌, 우\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "# (5) 정책 초기화\n",
    "policy = np.ones((grid_size, grid_size, len(actions))) / len(actions)\n",
    "for i, j in terminal_states:\n",
    "    policy[i, j] = np.zeros(len(actions))  # 종료 상태는 정책 없음\n",
    "\n",
    "# (6) 정책 평가 함수 정의: 정해진 정책 하에서 각 상태의 기대 보상(value)을 벨만 방정식을 기반으로 반복 계산\n",
    "def policy_evaluation(value_table, policy, iterations=100):\n",
    "    for _ in range(iterations):\n",
    "        new_value_table = np.copy(value_table) #상태 가치 값을 새롭게 갱신하기 위해 복사본 생성\n",
    "\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if (i, j) in terminal_states: #종료상태는 업데이트 하지 않음(가치고정)\n",
    "                    continue\n",
    "                value = 0\n",
    "\n",
    "                for action_idx, action in enumerate(actions):        #현재 상태 (i, j)에서 각 행동(상, 하, 좌, 우)을 시도\n",
    "                    next_i, next_j = i + action[0], j + action[1]\n",
    "\n",
    "                    # 경계를 벗어나면 현재 위치 유지\n",
    "                    if next_i < 0 or next_i >= grid_size or next_j < 0 or next_j >= grid_size:\n",
    "                        next_i, next_j = i, j\n",
    "\n",
    "                    reward = -1\n",
    "                    prob = policy[i, j, action_idx]\n",
    "                    value += prob * (reward + gamma * value_table[next_i, next_j]) #상태가치 벨만방정식\n",
    "\n",
    "                new_value_table[i, j] = value\n",
    "\n",
    "        value_table = new_value_table\n",
    "\n",
    "    return value_table\n",
    "\n",
    "# (7) 정책 개선 함수 정의:  주어진 상태 가치 함수 value_table을 기준으로 탐욕적(greedy)으로 최적 행동(액션)을 선택하여 정책 생성\n",
    "def policy_improvement(value_table):\n",
    "    new_policy = np.zeros((grid_size, grid_size, len(actions))) #정책 초기화\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            if (i, j) in terminal_states:\n",
    "                continue\n",
    "\n",
    "            values = []\n",
    "            for action in actions:\n",
    "                next_i, next_j = i + action[0], j + action[1]\n",
    "\n",
    "                if next_i < 0 or next_i >= grid_size or next_j < 0 or next_j >= grid_size:\n",
    "                    next_i, next_j = i, j\n",
    "\n",
    "                values.append(value_table[next_i, next_j]) #각 행동을 했을 때 도달하는 상태의 가치를 수집\n",
    "\n",
    "            best_action_value = np.max(values)             #가장 높은 상태 가치로 이동하는 행동을 선택\n",
    "            best_actions = [idx for idx, v in enumerate(values) if v == best_action_value] # 가장 높은 가치로 이동할 수 있는 행동 찾기\n",
    "            \n",
    "            # 탐욕적(Greedy) 행동 선택: 가장 좋은 행동에만 확률을 부여함 (탐욕적 선택) ex:policy[1, 2] = [0, 1, 0, 0] \n",
    "            for action_idx in best_actions:\n",
    "                new_policy[i, j, action_idx] = 1 / len(best_actions) #여러 개일 경우 확률을 나눠서 부여함 (예: 2개면 각 0.5)\n",
    "\n",
    "    return new_policy\n",
    "\n",
    "# (8) 정책 반복(평가 및 개선 반복 수행)\n",
    "def policy_iteration(iterations=10):\n",
    "    global value_table, policy\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        value_table = policy_evaluation(value_table, policy)\n",
    "        policy = policy_improvement(value_table)\n",
    "\n",
    "    return value_table, policy\n",
    "\n",
    "# 실행 및 결과 출력\n",
    "final_values, final_policy = policy_iteration()\n",
    "\n",
    "# (9) 최종 상태 가치 테이블\n",
    "print(\"최종 상태 가치 테이블:\")\n",
    "print(np.round(final_values, 2))\n",
    "\n",
    "# (10) 최정 정책\n",
    "print(\"\\n최종 정책(각 상태에서의 행동 확률):\")\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        print(f\"상태 ({i},{j}): {final_policy[i,j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c82b9-a1a4-4d1e-bae6-5d41e2c58a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
