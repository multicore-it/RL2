{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c2c6d8-e9e1-4803-8e09-c671429aa108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -22.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 547        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 74         |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00571423 |\n",
      "|    clip_fraction        | 0.00112    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.22      |\n",
      "|    explained_variance   | 0.336      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 3.73       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00674   |\n",
      "|    value_loss           | 21.4       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -23.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 559          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 146          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024555502 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.22        |\n",
      "|    explained_variance   | 0.534        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 1.57         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    value_loss           | 13.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -21.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 552          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 222          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066949073 |\n",
      "|    clip_fraction        | 0.0022       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.19        |\n",
      "|    explained_variance   | 0.661        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 1.78         |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00527     |\n",
      "|    value_loss           | 9.6          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -22.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012266668 |\n",
      "|    clip_fraction        | 0.0839      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.21       |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00993    |\n",
      "|    value_loss           | 9.65        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -20.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004754264 |\n",
      "|    clip_fraction        | 0.0181      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.18       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.606       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.00303    |\n",
      "|    value_loss           | 6.03        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -19        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 545        |\n",
      "|    iterations           | 120        |\n",
      "|    time_elapsed         | 450        |\n",
      "|    total_timesteps      | 245760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01252351 |\n",
      "|    clip_fraction        | 0.088      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.2       |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 1.12       |\n",
      "|    n_updates            | 1190       |\n",
      "|    policy_gradient_loss | -0.00908   |\n",
      "|    value_loss           | 7.69       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -18.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 547          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 523          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067552086 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.16        |\n",
      "|    explained_variance   | 0.751        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 0.862        |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | -0.00441     |\n",
      "|    value_loss           | 6.09         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -17.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 602         |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010869373 |\n",
      "|    clip_fraction        | 0.0585      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.522       |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    value_loss           | 3.47        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -17.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010819849 |\n",
      "|    clip_fraction        | 0.0636      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.14       |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.664       |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.00743    |\n",
      "|    value_loss           | 4.18        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -17         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 200         |\n",
      "|    time_elapsed         | 749         |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010347681 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.14       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.248       |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -16.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 547         |\n",
      "|    iterations           | 220         |\n",
      "|    time_elapsed         | 822         |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010439481 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.14       |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.37        |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.00918    |\n",
      "|    value_loss           | 3.68        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -16.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 549         |\n",
      "|    iterations           | 240         |\n",
      "|    time_elapsed         | 893         |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009838078 |\n",
      "|    clip_fraction        | 0.0741      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.12       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.713       |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | -0.00865    |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -16.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 553         |\n",
      "|    iterations           | 260         |\n",
      "|    time_elapsed         | 962         |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010445829 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.1        |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.403       |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -15.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 280         |\n",
      "|    time_elapsed         | 1032        |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008992997 |\n",
      "|    clip_fraction        | 0.0534      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.13       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -15.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 554        |\n",
      "|    iterations           | 300        |\n",
      "|    time_elapsed         | 1108       |\n",
      "|    total_timesteps      | 614400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01134588 |\n",
      "|    clip_fraction        | 0.0886     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.09      |\n",
      "|    explained_variance   | 0.832      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.202      |\n",
      "|    n_updates            | 2990       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    value_loss           | 2.2        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -15.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 320         |\n",
      "|    time_elapsed         | 1198        |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008934642 |\n",
      "|    clip_fraction        | 0.0805      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.12       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.166       |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | -0.00392    |\n",
      "|    value_loss           | 1.53        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -15.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 340         |\n",
      "|    time_elapsed         | 1274        |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008296602 |\n",
      "|    clip_fraction        | 0.0687      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.07       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | -0.00763    |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -14.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 544        |\n",
      "|    iterations           | 360        |\n",
      "|    time_elapsed         | 1353       |\n",
      "|    total_timesteps      | 737280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00853579 |\n",
      "|    clip_fraction        | 0.0684     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.08      |\n",
      "|    explained_variance   | 0.778      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.269      |\n",
      "|    n_updates            | 3590       |\n",
      "|    policy_gradient_loss | -0.00924   |\n",
      "|    value_loss           | 2.73       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -14.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 380         |\n",
      "|    time_elapsed         | 1432        |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010700239 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.08       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -14.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 400         |\n",
      "|    time_elapsed         | 1506        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011143302 |\n",
      "|    clip_fraction        | 0.0768      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 1.79        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -14.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 420         |\n",
      "|    time_elapsed         | 1579        |\n",
      "|    total_timesteps      | 860160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008298248 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0493      |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    value_loss           | 1.49        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -14.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 440         |\n",
      "|    time_elapsed         | 1663        |\n",
      "|    total_timesteps      | 901120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007835933 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0931      |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | -0.00728    |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -13.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 460          |\n",
      "|    time_elapsed         | 1739         |\n",
      "|    total_timesteps      | 942080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059367446 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.05        |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 0.00706      |\n",
      "|    n_updates            | 4590         |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    value_loss           | 1.07         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -13.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 480        |\n",
      "|    time_elapsed         | 1807       |\n",
      "|    total_timesteps      | 983040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00897925 |\n",
      "|    clip_fraction        | 0.0806     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.03      |\n",
      "|    explained_variance   | 0.874      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.041      |\n",
      "|    n_updates            | 4790       |\n",
      "|    policy_gradient_loss | -0.00869   |\n",
      "|    value_loss           | 1.11       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -13.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 500         |\n",
      "|    time_elapsed         | 1874        |\n",
      "|    total_timesteps      | 1024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009141218 |\n",
      "|    clip_fraction        | 0.091       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2          |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0621     |\n",
      "|    n_updates            | 4990        |\n",
      "|    policy_gradient_loss | -0.0079     |\n",
      "|    value_loss           | 1.02        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -13.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 548          |\n",
      "|    iterations           | 520          |\n",
      "|    time_elapsed         | 1941         |\n",
      "|    total_timesteps      | 1064960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060538976 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.03        |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0544      |\n",
      "|    n_updates            | 5190         |\n",
      "|    policy_gradient_loss | -0.000848    |\n",
      "|    value_loss           | 0.918        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -13         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 549         |\n",
      "|    iterations           | 540         |\n",
      "|    time_elapsed         | 2011        |\n",
      "|    total_timesteps      | 1105920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009359336 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.04       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0333      |\n",
      "|    n_updates            | 5390        |\n",
      "|    policy_gradient_loss | -0.00607    |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -13         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 550         |\n",
      "|    iterations           | 560         |\n",
      "|    time_elapsed         | 2083        |\n",
      "|    total_timesteps      | 1146880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008057587 |\n",
      "|    clip_fraction        | 0.0684      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.98       |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0708      |\n",
      "|    n_updates            | 5590        |\n",
      "|    policy_gradient_loss | -0.00896    |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -12.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 552          |\n",
      "|    iterations           | 580          |\n",
      "|    time_elapsed         | 2150         |\n",
      "|    total_timesteps      | 1187840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069799055 |\n",
      "|    clip_fraction        | 0.0673       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.99        |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0915      |\n",
      "|    n_updates            | 5790         |\n",
      "|    policy_gradient_loss | -0.00756     |\n",
      "|    value_loss           | 0.695        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -12.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 553         |\n",
      "|    iterations           | 600         |\n",
      "|    time_elapsed         | 2221        |\n",
      "|    total_timesteps      | 1228800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008071276 |\n",
      "|    clip_fraction        | 0.0949      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.01       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.162      |\n",
      "|    n_updates            | 5990        |\n",
      "|    policy_gradient_loss | -0.00774    |\n",
      "|    value_loss           | 0.583       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -12.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 553        |\n",
      "|    iterations           | 620        |\n",
      "|    time_elapsed         | 2295       |\n",
      "|    total_timesteps      | 1269760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00995245 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.01      |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.138     |\n",
      "|    n_updates            | 6190       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    value_loss           | 0.492      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 554         |\n",
      "|    iterations           | 640         |\n",
      "|    time_elapsed         | 2365        |\n",
      "|    total_timesteps      | 1310720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006093004 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.99       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.117      |\n",
      "|    n_updates            | 6390        |\n",
      "|    policy_gradient_loss | -0.00622    |\n",
      "|    value_loss           | 0.615       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -12.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 556          |\n",
      "|    iterations           | 660          |\n",
      "|    time_elapsed         | 2426         |\n",
      "|    total_timesteps      | 1351680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043772664 |\n",
      "|    clip_fraction        | 0.041        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0942      |\n",
      "|    n_updates            | 6590         |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 0.529        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -12.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 559         |\n",
      "|    iterations           | 680         |\n",
      "|    time_elapsed         | 2488        |\n",
      "|    total_timesteps      | 1392640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008886259 |\n",
      "|    clip_fraction        | 0.0836      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.117      |\n",
      "|    n_updates            | 6790        |\n",
      "|    policy_gradient_loss | -0.00724    |\n",
      "|    value_loss           | 0.533       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -12.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 562         |\n",
      "|    iterations           | 700         |\n",
      "|    time_elapsed         | 2550        |\n",
      "|    total_timesteps      | 1433600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008375665 |\n",
      "|    clip_fraction        | 0.0926      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.96       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0913     |\n",
      "|    n_updates            | 6990        |\n",
      "|    policy_gradient_loss | -0.00539    |\n",
      "|    value_loss           | 0.659       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -12.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 564         |\n",
      "|    iterations           | 720         |\n",
      "|    time_elapsed         | 2612        |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008910481 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.139      |\n",
      "|    n_updates            | 7190        |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 0.476       |\n",
      "-----------------------------------------\n",
      "**모델 학습 완료 (직급 분배 로직 없음)**\n"
     ]
    }
   ],
   "source": [
    "# 직원 순환 배치 PPO 강화학습\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. 실험 재현성을 위한 난수 시드(SEED) 고정\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# 2. 직원 및 지점 정보를 CSV 파일에서 로드\n",
    "employees = pd.read_csv(\"./data/rotation_employees.csv\")\n",
    "branches = pd.read_csv(\"./data/rotation_branches.csv\")\n",
    "\n",
    "# 3. 근무 기간에 따라 순환 배치 대상 직원(eligible)과 비대상 직원(non-eligible) 분류\n",
    "eligible_employees = employees[employees['years_at_branch'] > 3].reset_index(drop=True)\n",
    "non_eligible_employees = employees[employees['years_at_branch'] <= 3].copy()\n",
    "non_eligible_employees['new_branch'] = non_eligible_employees['current_branch']\n",
    "non_eligible_employees['reassigned'] = False\n",
    "\n",
    "# 4. 강화학습 환경에서 사용할 지점명 리스트 준비\n",
    "branch_names = branches['branch_name'].tolist()\n",
    "\n",
    "# 5. 직원의 집과 각 지점 간의 유클리드 거리를 계산하는 함수 정의하고, 이를 이용해 거리 행렬 생성\n",
    "def calculate_distance_matrix(emp_df, branch_df):\n",
    "    home_coords = emp_df[['home_x', 'home_y']].values\n",
    "    branch_coords = branch_df[['branch_x', 'branch_y']].values\n",
    "    return euclidean_distances(home_coords, branch_coords)\n",
    "    \n",
    "distance_matrix = calculate_distance_matrix(eligible_employees, branches)\n",
    "\n",
    "# 6. 직원 순환 배치 문제를 위한 사용자 정의 Gym 환경('RotationEnv') 정의\n",
    "class RotationEnv(gym.Env):\n",
    "\n",
    "    # 7. 환경 초기화 메서드: 필요한 데이터, 공간 정의, 상태 변수 초기화 등 수행\n",
    "    def __init__(self, emp_df, branch_df, distance_matrix, full_emp_df): # ranks 파라미터 제거\n",
    "        super().__init__()\n",
    "        self.emp_df = emp_df  # 이동 대상 직원\n",
    "        self.branch_df = branch_df # 지점 정보\n",
    "        self.distance_matrix = distance_matrix # 거리 행렬\n",
    "\n",
    "        self.n_emp = len(emp_df) # 이동 대상 직원 수\n",
    "        self.n_branches = len(branch_df) # 지점 수\n",
    "        self.max_branch_capacity = 12 # 지점 최대 정원\n",
    "        self.min_branch_capacity = 8  # 지점 최소 정원\n",
    "\n",
    "        # 7.1 에이전트가 배치함에 따라 변경될 지점별 인원 현황\n",
    "        self.branch_counts = np.zeros(self.n_branches, dtype=int)\n",
    "        self.current_idx = 0 # 현재 배치 중인 이동 대상 직원의 인덱스\n",
    "\n",
    "        # 7.2 이동 비대상 직원(고정 인원)으로 인한 초기 지점 상태(인원수) 설정\n",
    "        self.fixed_branch_counts = self._init_branch_state(full_emp_df) # _init_branch_state 반환값 변경에 따름\n",
    "\n",
    "        # 7.3 행동 공간(Action Space) 정의: 에이전트는 각 직원에 대해 n_branches개의 지점 중 하나 선택\n",
    "        self.action_space = spaces.Discrete(self.n_branches)\n",
    "        \n",
    "        # 7.4 관찰 공간(Observation Space) 정의: 직급 분포 정보 제거\n",
    "        # 구성: [현재 직원 집 좌표(2), 지점별 수용력 비율(n_branches)]\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(2 + self.n_branches,), dtype=np.float32)\n",
    "\n",
    "    # 8. (내부 메서드) 전체 직원 데이터를 기반으로 고정된 초기 지점 상태(총 인원) 계산\n",
    "    def _init_branch_state(self, full_df): # 반환값에서 rank_counts 제거\n",
    "        counts = np.zeros(self.n_branches, dtype=int)\n",
    "        for _, row in full_df.iterrows():\n",
    "            idx = self.branch_df[self.branch_df['branch_name'] == row['current_branch']].index[0]\n",
    "            counts[idx] += 1\n",
    "        return counts # counts만 반환\n",
    "\n",
    "    # 9. (Gym 표준 메서드) 환경의 난수 시드 설정\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "    # 10. (Gym 표준 메서드) 에피소드 시작 시 환경을 초기 상태로 리셋하고 첫 관찰 값 반환\n",
    "    def reset(self):\n",
    "        self.emp_df = self.emp_df.sample(frac=1).reset_index(drop=True) # 직원 순서 섞기\n",
    "        self.current_idx = 0\n",
    "        self.branch_counts[:] = 0 # 이번 에피소드 배치 현황 초기화\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    # 11. (내부 메서드) 현재 상태에 대한 관찰 값을 생성하여 에이전트에게 제공\n",
    "    def _get_obs(self):\n",
    "        emp = self.emp_df.iloc[self.current_idx]\n",
    "        home_xy = np.array([emp['home_x'] / 100.0, emp['home_y'] / 100.0])\n",
    "\n",
    "        total_counts = self.fixed_branch_counts + self.branch_counts\n",
    "        capacity_ratio = total_counts / self.max_branch_capacity\n",
    "\n",
    "        return np.concatenate([home_xy, capacity_ratio], axis=0).astype(np.float32)\n",
    "\n",
    "    # 12. (내부 메서드) 에이전트의 행동(action)에 대한 보상(reward) 계산. 직급 관련 패널티 로직 제거.\n",
    "    def _calculate_reward(self, action, emp): # rank 파라미터 제거\n",
    "        # 12-1 보상 점수를 매기기 위한 기초 정보를 준비\n",
    "        projected_count = self.fixed_branch_counts[action] + self.branch_counts[action] + 1 #그 지점의 예상 총 인원\n",
    "        total_required = self.min_branch_capacity * self.n_branches                         #모든 지점에 필요한 최소 총 인원\n",
    "        assigned_so_far = self.current_idx + 1                                              #지금까지 보낸 직원 수\n",
    "        progress_ratio = assigned_so_far / total_required if total_required > 0 else 0      #전체 배치 중 얼마나 진행됐는지 비율\n",
    "\n",
    "        # 12-2 필수 이동 대상자(4년 이상 근무)가 원래 지점에 남는 경우 패널티 (배치 진행률 60% 이상 시 적용)\n",
    "        moved_required_penalty = 0\n",
    "        if emp['years_at_branch'] >= 4 and emp['current_branch'] == self.branch_df.iloc[action]['branch_name']:\n",
    "            if progress_ratio >= 0.6:\n",
    "                moved_required_penalty = 1\n",
    "\n",
    "        # 12-3 정원 초과 & 미달 패널티 설정\n",
    "        over_penalty = 0\n",
    "        under_penalty = 0\n",
    "        if projected_count > self.max_branch_capacity: # 정원 초과 패널티\n",
    "            over_penalty = projected_count - self.max_branch_capacity\n",
    "        elif projected_count < self.min_branch_capacity: # 정원 미달 패널티 (배치 진행률 60% 이상 시 적용)\n",
    "            if progress_ratio >= 0.6:\n",
    "                under_penalty = self.min_branch_capacity - projected_count\n",
    "        \n",
    "        # 12-4 최종 보상 계산: 각 패널티 항목에 가중치 부여\n",
    "        reward = \\\n",
    "            - 1 * over_penalty  \\\n",
    "            - 4 * under_penalty  \\\n",
    "            - 2 * moved_required_penalty\n",
    "        return reward\n",
    "\n",
    "    # 13. (Gym 표준 메서드) 에이전트가 행동을 취하면, 환경 상태를 업데이트하고 다음 관찰, 보상, 종료 여부 등을 반환\n",
    "    def step(self, action):\n",
    "        # 13.1 현재 직원 정보 가져오고, 선택한 행동에 대한 점수 매기기\n",
    "        emp = self.emp_df.iloc[self.current_idx]\n",
    "        reward = self._calculate_reward(action, emp)\n",
    "\n",
    "        # 13.2 선택된 지점에 현재 직원 배치 및 상태 업데이트\n",
    "        self.branch_counts[action] += 1\n",
    "        self.current_idx += 1 # 다음 직원으로 이동\n",
    "\n",
    "        # 13.3 모든 이동 대상 직원 배치 완료 시 에피소드 종료\n",
    "        done = self.current_idx >= self.n_emp  \n",
    "\n",
    "        #13.4 다음 상황판 정보 만들고, 이번 행동의 최종 결과들 돌려주기\n",
    "        obs = self._get_obs() if not done else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# 14. 정의한 RotationEnv 환경의 인스턴스 생성\n",
    "env = RotationEnv(eligible_employees, branches, distance_matrix, full_emp_df=employees)\n",
    "\n",
    "# 15. PPO 모델의 정책 신경망 아키텍처 및 활성화 함수 등 추가 설정 정의\n",
    "policy_kwargs = dict(activation_fn=torch.nn.SiLU, \n",
    "                     net_arch=[dict(pi=[128,64], vf=[128,32])] # 정책망(pi), 가치망(vf) 구조\n",
    "                    )\n",
    "\n",
    "# 16. PPO 강화학습 모델 초기화. 주요 하이퍼파라미터(학습률, 엔트로피 계수 등) 설정\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", # 다층 퍼셉트론(MLP) 기반 정책 사용\n",
    "    env, \n",
    "    learning_rate=0.00005, # 학습률\n",
    "    ent_coef=0.1,          # 엔트로피 계수 (탐험 장려)\n",
    "    clip_range=0.2,        # PPO 클리핑 범위\n",
    "    vf_coef=0.2,           # 가치 함수 손실 가중치\n",
    "    policy_kwargs=policy_kwargs, # 신경망 설정\n",
    "    seed=SEED,             # 재현성을 위한 시드\n",
    "    verbose=1              # 학습 진행 상황 출력 레벨\n",
    ")\n",
    "\n",
    "# 17. PPO 모델 학습 시작\n",
    "model.learn(total_timesteps=1_500_000, log_interval=20) # 총 학습 스텝 수, 로그 출력 간격\n",
    "\n",
    "# 18. 학습된 모델 파일로 저장\n",
    "model.save(\"./models/rotation_ppo_model\") # 저장 파일명 변경하여 구분\n",
    "print(\"**모델 학습 완료**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd9c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4, 0, 5, 7, 8, 1, 8, 5, 2, 0, 4, 2, 8, 7, 5, 6, 7, 3, 8, 7, 2, 4, 7, 8, 3, 5, 2, 0, 8, 7, *최종 발령표 저장 완료: ./data/rotation_assignments_20250519_185305.csv\n",
      "*지점별 인원배분 점검 Old (직급별 + 총합)\n",
      "rank            과장  대리  사원  합계\n",
      "current_branch                \n",
      "Branch_0         4   3   3  10\n",
      "Branch_1         4   4   3  11\n",
      "Branch_2         4   3   3  10\n",
      "Branch_3         4   4   3  11\n",
      "Branch_4         4   3   3  10\n",
      "Branch_5         3   3   3   9\n",
      "Branch_6         4   4   3  11\n",
      "Branch_7         3   3   2   8\n",
      "Branch_8         3   3   2   8\n",
      "Branch_9         4   4   4  12\n",
      "*지점별 인원배분 점검 New (직급별 + 총합)\n",
      "rank        과장  대리  사원  합계\n",
      "new_branch                \n",
      "Branch_0     3   2   4   9\n",
      "Branch_1     5   2   0   7\n",
      "Branch_2     3   4   4  11\n",
      "Branch_3     4   5   3  12\n",
      "Branch_4     2   5   3  10\n",
      "Branch_5     3   5   2  10\n",
      "Branch_6     2   3   3   8\n",
      "Branch_7     6   3   3  12\n",
      "Branch_8     5   3   4  12\n",
      "Branch_9     4   2   3   9\n",
      "*4년 이상 근무한 '이동 대상' 직원의 부서 이동 여부:\n",
      "moved\n",
      "이동함      25\n",
      "이동 안함     5\n",
      "Name: count, dtype: int64\n",
      "*이동하지 않은 4년 이상 근무자 (이동 대상이었던 직원):\n",
      "    employee_id current_branch new_branch  years_at_branch rank\n",
      "16            3       Branch_7   Branch_7                4   과장\n",
      "12           14       Branch_8   Branch_8                8   대리\n",
      "14           41       Branch_5   Branch_5               10   사원\n",
      "8            42       Branch_2   Branch_2                6   과장\n",
      "6            97       Branch_8   Branch_8                7   대리\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 예측 및 결과 저장\n",
    "# ---------------------------\n",
    "# 1. 이전에 학습하고 저장한 PPO 모델 로딩\n",
    "model = PPO.load(\"./models/rotation_ppo_model\")\n",
    "\n",
    "# 2. 평가(예측)를 위해 직원 및 지점 정보 로드\n",
    "employees = pd.read_csv(\"./data/rotation_employees.csv\")  # 직원 정보\n",
    "branches = pd.read_csv(\"./data/rotation_branches.csv\")    # 지점 정보\n",
    "\n",
    "# 3. PyTorch가 결정론적 알고리즘을 사용하도록 설정\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# 4. 이동 대상 직원 목록을 특정 random_state로 재구성\n",
    "eligible_employees = eligible_employees.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 5. 평가를 위한 RotationEnv 환경 인스턴스 생성 및 환경 리셋\n",
    "env_eval = RotationEnv(eligible_employees, branches, distance_matrix, full_emp_df=employees)\n",
    "obs = env_eval.reset()\n",
    "\n",
    "# 6. 각 이동 대상 직원에 대한 예측된 배치 결과를 저장할 리스트 초기화\n",
    "assignments = []\n",
    "\n",
    "while True:\n",
    "    # 7. 학습된 모델을 사용하여 현재 관찰(obs)에 대한 최적의 행동(action, 즉 배치할 지점)을 예측\n",
    "    action, _ = model.predict(obs, deterministic=False) \n",
    "    \n",
    "    # 8. 예측된 행동(지점 인덱스) 출력\n",
    "    print(action, end=', ')\n",
    "    \n",
    "    # 9. 예측된 행동을 assignments 리스트 추가\n",
    "    assignments.append(int(action))\n",
    "    \n",
    "    # 10. 환경에서 해당 행동을 실행하고, 다음 관찰(obs), 보상(_), 종료 여부(done) 등의 정보 수신\n",
    "    obs, _, done, _ = env_eval.step(action)\n",
    "    \n",
    "    # 11. 모든 직원이 배치되었으면(done=True) 루프 종료\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# 12. 이동 대상 직원(eligible_employees) DataFrame에 예측된 'new_branch'(새 지점) 정보 추가\n",
    "eligible_employees_eval['new_branch'] = [branch_names[a] for a in assignments]\n",
    "\n",
    "# 13. 이동 대상 직원들은 재배치되었음을 표시\n",
    "eligible_employees_eval['reassigned'] = True\n",
    "\n",
    "# 14. 이동 대상 직원 결과와 비대상 직원 정보를 합쳐 최종 배치 DataFrame 생성하고, 직원 ID 순으로 정렬\n",
    "final_df = pd.concat([eligible_employees_eval, non_eligible_employees], ignore_index=True)\\\n",
    ".sort_values(by='employee_id')\n",
    "\n",
    "# 15. 현재 날짜와 시간을 포함한 파일명으로 최종 배치 결과를 CSV 파일로 저장\n",
    "filename = f\"./data/rotation_assignments_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "final_df.to_csv(filename, index=False, encoding='utf-8-sig') # UTF-8(BOM) 인코딩으로 Excel 호환성 향상\n",
    "\n",
    "print(f\"*최종 발령표 저장 완료: {filename}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 결과 점검 출력\n",
    "# ---------------------------\n",
    "# 16. 순환 배치 전(Old) 지점별, 직급별 인원 현황을 집계하여 출력\n",
    "branch_summary_old = final_df.groupby(['current_branch', 'rank']).size().unstack(fill_value=0)\n",
    "branch_summary_old['합계'] = branch_summary_old.sum(axis=1)\n",
    "print(\"*지점별 인원배분 점검 Old (직급별 + 총합)\")\n",
    "print(branch_summary_old)\n",
    "\n",
    "# 17. 순환 배치 후(New) 지점별, 직급별 인원 현황을 집계하여 출력\n",
    "branch_summary_new = final_df.groupby(['new_branch', 'rank']).size().unstack(fill_value=0)\n",
    "branch_summary_new['합계'] = branch_summary_new.sum(axis=1)\n",
    "print(\"*지점별 인원배분 점검 New (직급별 + 총합)\")\n",
    "print(branch_summary_new)\n",
    "\n",
    "moved_4yrs_eligible_ids = eligible_employees_eval['employee_id']\n",
    "moved_4yrs_check_df = final_df[(final_df['employee_id'].\\\n",
    "                                isin(moved_4yrs_eligible_ids)) & (final_df['years_at_branch'] >= 4)].copy()\n",
    "\n",
    "if not moved_4yrs_check_df.empty:\n",
    "    # 18. 이동 대상이면서 4년 이상 근무한 직원들의 실제 이동 여부 분석\n",
    "    moved_4yrs_check_df['moved'] = moved_4yrs_check_df['current_branch'] != moved_4yrs_check_df['new_branch']\n",
    "    moved_summary = moved_4yrs_check_df['moved'].value_counts(dropna=False)\\\n",
    "    .rename(index={True: '이동함', False: '이동 안함', np.nan: '해당없음'})\n",
    "    print(\"*4년 이상 근무한 '이동 대상' 직원의 부서 이동 여부:\")\n",
    "    print(moved_summary)\n",
    "\n",
    "    # 19. 만약 이동해야 하는 4년 이상 근무자 중 이동하지 않은 인원이 있다면, 해당 직원 정보 출력\n",
    "    not_moved_list = moved_4yrs_check_df[moved_4yrs_check_df['moved'] == False]\n",
    "    if not not_moved_list.empty:\n",
    "        print(\"*이동하지 않은 4년 이상 근무자 (이동 대상이었던 직원):\")\n",
    "        print(not_moved_list[['employee_id', 'current_branch', 'new_branch', 'years_at_branch', 'rank']])\n",
    "    else:\n",
    "        print(\"*모든 4년 이상 '이동 대상' 근무자는 부서 이동 완료 또는 대상자 없음\")\n",
    "else:\n",
    "    print(\"*4년 이상 근무한 '이동 대상' 직원이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9e01e6c-0234-4197-96b7-7c9deb6bd5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "과거 지점과 현재 배정 지점 간 거리 평균 비교:\n",
      "*과거 지점 평균 거리: 49.07\n",
      "*현재 배정 지점 평균 거리: 47.75\n",
      "*평균적으로 직원들의 출퇴근 거리가 1.32만큼 줄었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 1. 지점의 문자열 명칭과 해당 지점 데이터의 숫자 인덱스 간의 상호 참조 가능한 매핑 구조 생성\n",
    "branch_name_to_idx = {name: idx for idx, name in enumerate(branch_names)}\n",
    "\n",
    "# 2. 순환 배치 적용 이전과 이후, 각 직원의 자택과 해당 근무 지점 간의 유클리드 거리를\n",
    "# 개별적으로 저장하기 위한 리스트 변수를 초기화\n",
    "past_distances, current_distances = [], []\n",
    "\n",
    "# 3. 최종적으로 확정된 직원 배치 결과('final_df')를 한 명씩 순회하면서,\n",
    "# 각 직원의 배치 전후 근무지와 자택 간의 거리 변화를 정량적으로 산출\n",
    "for _, row in final_df.iterrows():\n",
    "    \n",
    "    # 3-1. 현재 순회 중인 직원의 자택 위치 좌표(x, y)를 NumPy 배열 형태로 추출\n",
    "    home = np.array([row['home_x'], row['home_y']])\n",
    "    \n",
    "    # 3-2. 해당 직원의 순환 배치 이전 근무 지점명과 새로 배정된 지점명을 사용하여,\n",
    "    # 앞서 생성한 'branch_name_to_idx' 매핑 테이블을 통해 각 지점의 고유 숫자 인덱스 조회\n",
    "    past_idx = branch_name_to_idx[row['current_branch']]\n",
    "    curr_idx = branch_name_to_idx[row['new_branch']]\n",
    "    \n",
    "    # 3-3. 조회된 숫자 인덱스를 활용하여 전체 지점 정보가 담긴 'branches' DataFrame에서\n",
    "    # 해당 지점들의 물리적 위치 좌표(x, y) 추출\n",
    "    past_loc = branches.iloc[past_idx][['branch_x', 'branch_y']].values\n",
    "    curr_loc = branches.iloc[curr_idx][['branch_x', 'branch_y']].values\n",
    "\n",
    "    # 3-4. 직원의 자택 좌표와 각 근무 지점(순환 배치 이전 및 현재)의 좌표 간의\n",
    "    # 직선 거리, 즉 유클리드 거리를 'np.linalg.norm' 함수를 이용하여 계산\n",
    "    # 이 함수는 두 벡터 차의 L2 노름(norm)을 계산함으로써 두 점 사이의 거리 나타냄\n",
    "    past_distances.append(np.linalg.norm(home - past_loc))\n",
    "    current_distances.append(np.linalg.norm(home - curr_loc))\n",
    "\n",
    "# 4. 전 직원을 대상으로 산출된, 순환 배치 이전/이후 근무 지점까지의 개별 거리들의 산술 평균 계산\n",
    "past_mean = np.mean(past_distances)\n",
    "curr_mean = np.mean(current_distances)\n",
    "\n",
    "# 5. 두 평균 거리 값의 차이를 계산하여, 이번 순환 배치가 전체 직원의 평균적인\n",
    "# 자택-근무지 간 거리에 어떠한 변화를 야기했는지를 정량적으로 나타냄\n",
    "diff = curr_mean - past_mean\n",
    "\n",
    "print(\"\\n과거 지점과 현재 배정 지점 간 거리 평균 비교:\")\n",
    "print(f\"*과거 지점 평균 거리: {past_mean:.2f}\")\n",
    "print(f\"*현재 배정 지점 평균 거리: {curr_mean:.2f}\")\n",
    "\n",
    "if diff < 0:\n",
    "    print(f\"*평균적으로 직원들의 출퇴근 거리가 {abs(diff):.2f}만큼 줄었습니다.\")\n",
    "else:\n",
    "    print(f\"*평균적으로 직원들의 출퇴근 거리가 {diff:.2f}만큼 늘었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ac6ff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, ❌ 조건 불만족 (시도 1): 지점별 인원 범위(8~12명) 이탈\n",
      "2, ❌ 조건 불만족 (시도 2): 지점별 인원 범위(8~12명) 이탈\n",
      "3, ❌ 조건 불만족 (시도 3): 지점별 인원 범위(8~12명) 이탈\n",
      "4, ❌ 조건 불만족 (시도 4): 지점별 인원 범위(8~12명) 이탈\n",
      "5, ❌ 조건 불만족 (시도 5): 지점별 인원 범위(8~12명) 이탈\n",
      "6, ❌ 조건 불만족 (시도 6): 지점별 인원 범위(8~12명) 이탈\n",
      "7, ❌ 조건 불만족 (시도 7): 지점별 인원 범위(8~12명) 이탈\n",
      "8, ❌ 조건 불만족 (시도 8): 지점별 인원 범위(8~12명) 이탈\n",
      "9, ❌ 조건 불만족 (시도 9): 지점별 인원 범위(8~12명) 이탈\n",
      "10, ❌ 조건 불만족 (시도 10): 지점별 인원 범위(8~12명) 이탈\n",
      "11, ❌ 조건 불만족 (시도 11): 지점별 인원 범위(8~12명) 이탈\n",
      "12, ❌ 조건 불만족 (시도 12): 지점별 인원 범위(8~12명) 이탈\n",
      "13, ❌ 조건 불만족 (시도 13): 지점별 인원 범위(8~12명) 이탈\n",
      "14, ❌ 조건 불만족 (시도 14): 지점별 인원 범위(8~12명) 이탈\n",
      "15, ❌ 조건 불만족 (시도 15): 지점별 인원 범위(8~12명) 이탈\n",
      "16, ❌ 조건 불만족 (시도 16): 지점별 인원 범위(8~12명) 이탈\n",
      "17, ❌ 조건 불만족 (시도 17): 지점별 인원 범위(8~12명) 이탈\n",
      "18, ❌ 조건 불만족 (시도 18): 지점별 인원 범위(8~12명) 이탈\n",
      "19, ❌ 조건 불만족 (시도 19): 지점별 인원 범위(8~12명) 이탈\n",
      "20, ❌ 조건 불만족 (시도 20): 지점별 인원 범위(8~12명) 이탈\n",
      "21, ❌ 조건 불만족 (시도 21): 지점별 인원 범위(8~12명) 이탈\n",
      "22, ❌ 조건 불만족 (시도 22): 지점별 인원 범위(8~12명) 이탈\n",
      "23, ❌ 조건 불만족 (시도 23): 지점별 인원 범위(8~12명) 이탈\n",
      "24, ❌ 조건 불만족 (시도 24): 지점별 인원 범위(8~12명) 이탈\n",
      "25, ❌ 조건 불만족 (시도 25): 지점별 인원 범위(8~12명) 이탈\n",
      "26, ❌ 조건 불만족 (시도 26): 지점별 인원 범위(8~12명) 이탈\n",
      "27, ❌ 조건 불만족 (시도 27): 지점별 인원 범위(8~12명) 이탈\n",
      "28, ❌ 조건 불만족 (시도 28): 지점별 인원 범위(8~12명) 이탈\n",
      "29, ❌ 조건 불만족 (시도 29): 지점별 인원 범위(8~12명) 이탈\n",
      "30, ❌ 조건 불만족 (시도 30): 지점별 인원 범위(8~12명) 이탈\n",
      "31, ❌ 조건 불만족 (시도 31): 지점별 인원 범위(8~12명) 이탈\n",
      "32, ❌ 조건 불만족 (시도 32): 지점별 인원 범위(8~12명) 이탈\n",
      "33, ❌ 조건 불만족 (시도 33): 지점별 인원 범위(8~12명) 이탈\n",
      "34, ❌ 조건 불만족 (시도 34): 지점별 인원 범위(8~12명) 이탈\n",
      "35, ❌ 조건 불만족 (시도 35): 지점별 인원 범위(8~12명) 이탈\n",
      "36, ❌ 조건 불만족 (시도 36): 지점별 인원 범위(8~12명) 이탈\n",
      "37, ❌ 조건 불만족 (시도 37): 지점별 인원 범위(8~12명) 이탈\n",
      "38, ❌ 조건 불만족 (시도 38): 지점별 인원 범위(8~12명) 이탈\n",
      "39, ❌ 조건 불만족 (시도 39): 지점별 인원 범위(8~12명) 이탈\n",
      "40, ❌ 조건 불만족 (시도 40): 지점별 인원 범위(8~12명) 이탈\n",
      "41, ❌ 조건 불만족 (시도 41): 지점별 인원 범위(8~12명) 이탈\n",
      "42, ❌ 조건 불만족 (시도 42): 지점별 인원 범위(8~12명) 이탈\n",
      "43, ❌ 조건 불만족 (시도 43): 지점별 인원 범위(8~12명) 이탈\n",
      "44, ❌ 조건 불만족 (시도 44): 지점별 인원 범위(8~12명) 이탈\n",
      "45, ❌ 조건 불만족 (시도 45): 지점별 인원 범위(8~12명) 이탈\n",
      "46, ❌ 조건 불만족 (시도 46): 지점별 인원 범위(8~12명) 이탈\n",
      "47, ❌ 조건 불만족 (시도 47): 지점별 인원 범위(8~12명) 이탈\n",
      "48, ❌ 조건 불만족 (시도 48): 지점별 인원 범위(8~12명) 이탈\n",
      "49, ❌ 조건 불만족 (시도 49): 지점별 인원 범위(8~12명) 이탈\n",
      "50, ❌ 조건 불만족 (시도 50): 4년 이상 근무자 미이동 발생\n",
      "51, ❌ 조건 불만족 (시도 51): 지점별 인원 범위(8~12명) 이탈\n",
      "52, ❌ 조건 불만족 (시도 52): 4년 이상 근무자 미이동 발생\n",
      "53, ❌ 조건 불만족 (시도 53): 지점별 인원 범위(8~12명) 이탈\n",
      "54, ❌ 조건 불만족 (시도 54): 지점별 인원 범위(8~12명) 이탈\n",
      "55, ❌ 조건 불만족 (시도 55): 지점별 인원 범위(8~12명) 이탈\n",
      "56, ❌ 조건 불만족 (시도 56): 지점별 인원 범위(8~12명) 이탈\n",
      "57, ❌ 조건 불만족 (시도 57): 지점별 인원 범위(8~12명) 이탈\n",
      "58, ❌ 조건 불만족 (시도 58): 지점별 인원 범위(8~12명) 이탈\n",
      "59, ❌ 조건 불만족 (시도 59): 지점별 인원 범위(8~12명) 이탈\n",
      "60, ❌ 조건 불만족 (시도 60): 지점별 인원 범위(8~12명) 이탈\n",
      "61, ❌ 조건 불만족 (시도 61): 지점별 인원 범위(8~12명) 이탈\n",
      "62, ❌ 조건 불만족 (시도 62): 지점별 인원 범위(8~12명) 이탈\n",
      "63, ❌ 조건 불만족 (시도 63): 지점별 인원 범위(8~12명) 이탈\n",
      "64, ❌ 조건 불만족 (시도 64): 지점별 인원 범위(8~12명) 이탈\n",
      "65, ❌ 조건 불만족 (시도 65): 지점별 인원 범위(8~12명) 이탈\n",
      "66, ❌ 조건 불만족 (시도 66): 지점별 인원 범위(8~12명) 이탈\n",
      "67, ❌ 조건 불만족 (시도 67): 지점별 인원 범위(8~12명) 이탈\n",
      "68, ❌ 조건 불만족 (시도 68): 지점별 인원 범위(8~12명) 이탈\n",
      "69, ❌ 조건 불만족 (시도 69): 지점별 인원 범위(8~12명) 이탈\n",
      "70, ❌ 조건 불만족 (시도 70): 지점별 인원 범위(8~12명) 이탈\n",
      "71, ❌ 조건 불만족 (시도 71): 지점별 인원 범위(8~12명) 이탈\n",
      "72, ❌ 조건 불만족 (시도 72): 지점별 인원 범위(8~12명) 이탈\n",
      "73, ❌ 조건 불만족 (시도 73): 지점별 인원 범위(8~12명) 이탈\n",
      "74, ❌ 조건 불만족 (시도 74): 지점별 인원 범위(8~12명) 이탈\n",
      "75, ❌ 조건 불만족 (시도 75): 지점별 인원 범위(8~12명) 이탈\n",
      "76, ❌ 조건 불만족 (시도 76): 지점별 인원 범위(8~12명) 이탈\n",
      "77, ❌ 조건 불만족 (시도 77): 4년 이상 근무자 미이동 발생\n",
      "78, ❌ 조건 불만족 (시도 78): 지점별 인원 범위(8~12명) 이탈\n",
      "79, ❌ 조건 불만족 (시도 79): 지점별 인원 범위(8~12명) 이탈\n",
      "80, ❌ 조건 불만족 (시도 80): 지점별 인원 범위(8~12명) 이탈\n",
      "81, ❌ 조건 불만족 (시도 81): 지점별 인원 범위(8~12명) 이탈\n",
      "82, ❌ 조건 불만족 (시도 82): 지점별 인원 범위(8~12명) 이탈\n",
      "83, ❌ 조건 불만족 (시도 83): 지점별 인원 범위(8~12명) 이탈\n",
      "84, ❌ 조건 불만족 (시도 84): 지점별 인원 범위(8~12명) 이탈\n",
      "85, ❌ 조건 불만족 (시도 85): 지점별 인원 범위(8~12명) 이탈\n",
      "86, ❌ 조건 불만족 (시도 86): 지점별 인원 범위(8~12명) 이탈\n",
      "87, ❌ 조건 불만족 (시도 87): 지점별 인원 범위(8~12명) 이탈\n",
      "88, ❌ 조건 불만족 (시도 88): 지점별 인원 범위(8~12명) 이탈\n",
      "89, ❌ 조건 불만족 (시도 89): 지점별 인원 범위(8~12명) 이탈\n",
      "90, ❌ 조건 불만족 (시도 90): 지점별 인원 범위(8~12명) 이탈\n",
      "91, ❌ 조건 불만족 (시도 91): 지점별 인원 범위(8~12명) 이탈\n",
      "92, ❌ 조건 불만족 (시도 92): 지점별 인원 범위(8~12명) 이탈\n",
      "93, ❌ 조건 불만족 (시도 93): 지점별 인원 범위(8~12명) 이탈\n",
      "94, ❌ 조건 불만족 (시도 94): 지점별 인원 범위(8~12명) 이탈\n",
      "95, ❌ 조건 불만족 (시도 95): 지점별 인원 범위(8~12명) 이탈\n",
      "96, ❌ 조건 불만족 (시도 96): 지점별 인원 범위(8~12명) 이탈\n",
      "97, ❌ 조건 불만족 (시도 97): 지점별 인원 범위(8~12명) 이탈\n",
      "98, ❌ 조건 불만족 (시도 98): 지점별 인원 범위(8~12명) 이탈\n",
      "99, ❌ 조건 불만족 (시도 99): 지점별 인원 범위(8~12명) 이탈\n",
      "100, ❌ 조건 불만족 (시도 100): 지점별 인원 범위(8~12명) 이탈\n",
      "101, ❌ 조건 불만족 (시도 101): 지점별 인원 범위(8~12명) 이탈\n",
      "102, ❌ 조건 불만족 (시도 102): 지점별 인원 범위(8~12명) 이탈\n",
      "103, ❌ 조건 불만족 (시도 103): 지점별 인원 범위(8~12명) 이탈\n",
      "104, ❌ 조건 불만족 (시도 104): 지점별 인원 범위(8~12명) 이탈\n",
      "105, ❌ 조건 불만족 (시도 105): 지점별 인원 범위(8~12명) 이탈\n",
      "106, ❌ 조건 불만족 (시도 106): 지점별 인원 범위(8~12명) 이탈\n",
      "107, ❌ 조건 불만족 (시도 107): 지점별 인원 범위(8~12명) 이탈\n",
      "108, ❌ 조건 불만족 (시도 108): 지점별 인원 범위(8~12명) 이탈\n",
      "109, ❌ 조건 불만족 (시도 109): 지점별 인원 범위(8~12명) 이탈\n",
      "110, ❌ 조건 불만족 (시도 110): 지점별 인원 범위(8~12명) 이탈\n",
      "111, ❌ 조건 불만족 (시도 111): 지점별 인원 범위(8~12명) 이탈\n",
      "112, ❌ 조건 불만족 (시도 112): 지점별 인원 범위(8~12명) 이탈\n",
      "113, ❌ 조건 불만족 (시도 113): 지점별 인원 범위(8~12명) 이탈\n",
      "114, ❌ 조건 불만족 (시도 114): 지점별 인원 범위(8~12명) 이탈\n",
      "115, ❌ 조건 불만족 (시도 115): 지점별 인원 범위(8~12명) 이탈\n",
      "116, ❌ 조건 불만족 (시도 116): 지점별 인원 범위(8~12명) 이탈\n",
      "117, ❌ 조건 불만족 (시도 117): 지점별 인원 범위(8~12명) 이탈\n",
      "118, ❌ 조건 불만족 (시도 118): 지점별 인원 범위(8~12명) 이탈\n",
      "119, ❌ 조건 불만족 (시도 119): 지점별 인원 범위(8~12명) 이탈\n",
      "120, ❌ 조건 불만족 (시도 120): 지점별 인원 범위(8~12명) 이탈\n",
      "121, ❌ 조건 불만족 (시도 121): 지점별 인원 범위(8~12명) 이탈\n",
      "122, ❌ 조건 불만족 (시도 122): 지점별 인원 범위(8~12명) 이탈\n",
      "123, ❌ 조건 불만족 (시도 123): 4년 이상 근무자 미이동 발생\n",
      "124, ❌ 조건 불만족 (시도 124): 지점별 인원 범위(8~12명) 이탈\n",
      "125, ❌ 조건 불만족 (시도 125): 지점별 인원 범위(8~12명) 이탈\n",
      "126, ❌ 조건 불만족 (시도 126): 지점별 인원 범위(8~12명) 이탈\n",
      "127, ❌ 조건 불만족 (시도 127): 지점별 인원 범위(8~12명) 이탈\n",
      "128, ❌ 조건 불만족 (시도 128): 지점별 인원 범위(8~12명) 이탈\n",
      "129, ❌ 조건 불만족 (시도 129): 지점별 인원 범위(8~12명) 이탈\n",
      "130, ❌ 조건 불만족 (시도 130): 지점별 인원 범위(8~12명) 이탈\n",
      "131, ❌ 조건 불만족 (시도 131): 지점별 인원 범위(8~12명) 이탈\n",
      "132, ❌ 조건 불만족 (시도 132): 지점별 인원 범위(8~12명) 이탈\n",
      "133, ❌ 조건 불만족 (시도 133): 지점별 인원 범위(8~12명) 이탈\n",
      "134, ❌ 조건 불만족 (시도 134): 지점별 인원 범위(8~12명) 이탈\n",
      "135, ❌ 조건 불만족 (시도 135): 지점별 인원 범위(8~12명) 이탈\n",
      "136, ❌ 조건 불만족 (시도 136): 지점별 인원 범위(8~12명) 이탈\n",
      "137, ❌ 조건 불만족 (시도 137): 지점별 인원 범위(8~12명) 이탈\n",
      "138, ❌ 조건 불만족 (시도 138): 지점별 인원 범위(8~12명) 이탈\n",
      "139, ❌ 조건 불만족 (시도 139): 지점별 인원 범위(8~12명) 이탈\n",
      "140, ❌ 조건 불만족 (시도 140): 지점별 인원 범위(8~12명) 이탈\n",
      "141, ❌ 조건 불만족 (시도 141): 지점별 인원 범위(8~12명) 이탈\n",
      "142, ❌ 조건 불만족 (시도 142): 4년 이상 근무자 미이동 발생\n",
      "143, ❌ 조건 불만족 (시도 143): 지점별 인원 범위(8~12명) 이탈\n",
      "144, ❌ 조건 불만족 (시도 144): 지점별 인원 범위(8~12명) 이탈\n",
      "145, ❌ 조건 불만족 (시도 145): 지점별 인원 범위(8~12명) 이탈\n",
      "146, ❌ 조건 불만족 (시도 146): 지점별 인원 범위(8~12명) 이탈\n",
      "147, ❌ 조건 불만족 (시도 147): 지점별 인원 범위(8~12명) 이탈\n",
      "148, ❌ 조건 불만족 (시도 148): 지점별 인원 범위(8~12명) 이탈\n",
      "149, ❌ 조건 불만족 (시도 149): 지점별 인원 범위(8~12명) 이탈\n",
      "150, ❌ 조건 불만족 (시도 150): 지점별 인원 범위(8~12명) 이탈\n",
      "151, ❌ 조건 불만족 (시도 151): 지점별 인원 범위(8~12명) 이탈\n",
      "152, ❌ 조건 불만족 (시도 152): 지점별 인원 범위(8~12명) 이탈\n",
      "153, ❌ 조건 불만족 (시도 153): 지점별 인원 범위(8~12명) 이탈\n",
      "154, ❌ 조건 불만족 (시도 154): 지점별 인원 범위(8~12명) 이탈\n",
      "155, ❌ 조건 불만족 (시도 155): 지점별 인원 범위(8~12명) 이탈\n",
      "156, ❌ 조건 불만족 (시도 156): 지점별 인원 범위(8~12명) 이탈\n",
      "157, ❌ 조건 불만족 (시도 157): 지점별 인원 범위(8~12명) 이탈\n",
      "158, ❌ 조건 불만족 (시도 158): 지점별 인원 범위(8~12명) 이탈\n",
      "159, ❌ 조건 불만족 (시도 159): 지점별 인원 범위(8~12명) 이탈\n",
      "160, ❌ 조건 불만족 (시도 160): 지점별 인원 범위(8~12명) 이탈\n",
      "161, ❌ 조건 불만족 (시도 161): 지점별 인원 범위(8~12명) 이탈\n",
      "162, ❌ 조건 불만족 (시도 162): 지점별 인원 범위(8~12명) 이탈\n",
      "163, ❌ 조건 불만족 (시도 163): 지점별 인원 범위(8~12명) 이탈\n",
      "164, ❌ 조건 불만족 (시도 164): 지점별 인원 범위(8~12명) 이탈\n",
      "165, ❌ 조건 불만족 (시도 165): 지점별 인원 범위(8~12명) 이탈\n",
      "166, ❌ 조건 불만족 (시도 166): 지점별 인원 범위(8~12명) 이탈\n",
      "167, ❌ 조건 불만족 (시도 167): 지점별 인원 범위(8~12명) 이탈\n",
      "168, ❌ 조건 불만족 (시도 168): 지점별 인원 범위(8~12명) 이탈\n",
      "169, ❌ 조건 불만족 (시도 169): 지점별 인원 범위(8~12명) 이탈\n",
      "170, ❌ 조건 불만족 (시도 170): 지점별 인원 범위(8~12명) 이탈\n",
      "171, ❌ 조건 불만족 (시도 171): 지점별 인원 범위(8~12명) 이탈\n",
      "172, ❌ 조건 불만족 (시도 172): 지점별 인원 범위(8~12명) 이탈\n",
      "173, ❌ 조건 불만족 (시도 173): 지점별 인원 범위(8~12명) 이탈\n",
      "174, ❌ 조건 불만족 (시도 174): 지점별 인원 범위(8~12명) 이탈\n",
      "175, ❌ 조건 불만족 (시도 175): 지점별 인원 범위(8~12명) 이탈\n",
      "176, ❌ 조건 불만족 (시도 176): 지점별 인원 범위(8~12명) 이탈\n",
      "177, ❌ 조건 불만족 (시도 177): 지점별 인원 범위(8~12명) 이탈\n",
      "178, ❌ 조건 불만족 (시도 178): 지점별 인원 범위(8~12명) 이탈\n",
      "179, ❌ 조건 불만족 (시도 179): 지점별 인원 범위(8~12명) 이탈\n",
      "180, ❌ 조건 불만족 (시도 180): 지점별 인원 범위(8~12명) 이탈\n",
      "181, ❌ 조건 불만족 (시도 181): 지점별 인원 범위(8~12명) 이탈\n",
      "182, \n",
      "✅ 조건 충족 완료! (시도 182) 결과 저장: ./data/rotation_assignments_final_20250519_191420.csv\n",
      "▶ 지점별 인원: {'Branch_8': 12, 'Branch_7': 12, 'Branch_3': 12, 'Branch_5': 10, 'Branch_4': 10, 'Branch_2': 10, 'Branch_0': 9, 'Branch_9': 9, 'Branch_1': 8, 'Branch_6': 8}\n",
      "▶ 4년 이상 근무자 수(이동 대상): 30명, 이동률: 100.00%\n",
      "▶ 평균 거리 변화 (이전-현재): -0.43 (양수면 거리 감소)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "각 지점 인원: 8~12명 사이\n",
    "4년 이상 근무자의 전원 이동\n",
    "평균 거리 감소 폭 ≥ 1 이상\n",
    "'''\n",
    "# 1. 최대 반복 시도 횟수 설정 및 성공 여부 플래그 초기화\n",
    "max_attempts = 1000\n",
    "success = False\n",
    "\n",
    "for attempt in range(1, max_attempts + 1):\n",
    "    print(attempt, end=\", \") # 현재 시도 횟수 출력\n",
    "\n",
    "    # 2. 각 시도마다 이동 대상 직원의 처리 순서를 무작위로 변경\n",
    "    np.random.seed(None) # Pandas sample의 무작위성 확보\n",
    "    eligible_employees_shuffled = eligible_employees.sample(frac=1).reset_index(drop=True)\n",
    "    np.random.seed(SEED) # 기타 NumPy 연산의 재현성 확보\n",
    "    \n",
    "    env_eval = RotationEnv(eligible_employees_shuffled, branches, distance_matrix, employees)\n",
    "    obs = env_eval.reset()\n",
    "    model = PPO.load(\"./models/rotation_ppo_model\")\n",
    "\n",
    "    # 3. 준비된 환경과 모델을 사용하여 이동 대상 직원들의 새로운 지점 배치를 순차적으로 예측\n",
    "    assignments = []\n",
    "    while True:\n",
    "        action, _ = model.predict(obs) # deterministic=False (기본값)으로 정책에 따른 샘플링\n",
    "        assignments.append(int(action))\n",
    "        obs, _, done, _ = env_eval.step(action) # 환경 상태 업데이트\n",
    "        if done: # 모든 직원이 배치되면 종료\n",
    "            break\n",
    "\n",
    "    # 4. 예측된 배치 결과를 바탕으로 최종 발령표(DataFrame)를 구성\n",
    "    eligible_employees_shuffled['new_branch'] = [branch_names[a] for a in assignments]\n",
    "    eligible_employees_shuffled['reassigned'] = True\n",
    "    final_df = pd.concat([eligible_employees_shuffled, non_eligible_employees], ignore_index=True)\\\n",
    "    .sort_values(by='employee_id')\n",
    "\n",
    "    # 5. 생성된 배치안이 사전에 정의된 운영상의 필수 조건들을 만족하는지 검증\n",
    "    # 5-1. 지점 인원 수 조건 검증\n",
    "    branch_counts = final_df['new_branch'].value_counts()\n",
    "    if not branch_counts.between(8, 12).all():\n",
    "        print(f\"❌ 조건 불만족 (시도 {attempt}): 지점별 인원 범위(8~12명) 이탈\")\n",
    "        continue\n",
    "\n",
    "    # 5-2. 4년 이상 근무자 이동 조건 검증\n",
    "    moved_4yrs_subset = eligible_employees_shuffled[eligible_employees_shuffled['years_at_branch'] >= 4].copy()\n",
    "    if not moved_4yrs_subset.empty:\n",
    "        moved_4yrs_subset['moved'] = moved_4yrs_subset['current_branch'] != moved_4yrs_subset['new_branch']\n",
    "        if not moved_4yrs_subset['moved'].all():\n",
    "            print(f\"❌ 조건 불만족 (시도 {attempt}): 4년 이상 근무자 미이동 발생\")\n",
    "            continue\n",
    "    \n",
    "    # 5-3. 평균 거리 조건 검증 (현재 로직: 새 평균 거리가 이전 평균보다 1 이상 크게 늘어나지 않아야 함)\n",
    "    past_distances, current_distances = [], []\n",
    "    branch_name_to_idx = {name: idx for idx, name in enumerate(branch_names)}\n",
    "    for _, row in final_df.iterrows():\n",
    "        home = np.array([row['home_x'], row['home_y']])\n",
    "        past_idx = branch_name_to_idx[row['current_branch']]\n",
    "        curr_idx = branch_name_to_idx[row['new_branch']]\n",
    "        past_loc = branches.iloc[past_idx][['branch_x', 'branch_y']].values\n",
    "        curr_loc = branches.iloc[curr_idx][['branch_x', 'branch_y']].values\n",
    "        past_distances.append(np.linalg.norm(home - past_loc))\n",
    "        current_distances.append(np.linalg.norm(home - curr_loc))\n",
    "    past_mean = np.mean(past_distances) if past_distances else 0\n",
    "    curr_mean = np.mean(current_distances) if current_distances else 0\n",
    "    diff_metric = past_mean - curr_mean # 이 값이 양수면 거리 감소\n",
    "\n",
    "    if diff_metric <= -1.0: # 현재 코드의 `if diff > -1.0: continue`의 실패 조건\n",
    "        print(f\"❌ 조건 불만족 (시도 {attempt}): 평균 거리 1 이상 증가 (변화량: {diff_metric:.2f})\")\n",
    "        continue\n",
    "\n",
    "    # 6. 모든 검증 조건을 성공적으로 통과한 경우, 결과를 저장하고 루프 종료\n",
    "    success = True\n",
    "    filename = f\"./data/rotation_assignments_final_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    final_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n✅ 조건 충족 완료! (시도 {attempt}) 결과 저장: {filename}\")\n",
    "    print(f\"▶ 지점별 인원: {branch_counts.to_dict()}\")\n",
    "    if not moved_4yrs_subset.empty:\n",
    "         moved_percentage = 100.0 if moved_4yrs_subset['moved'].all() else (moved_4yrs_subset['moved'].sum() / len(moved_4yrs_subset) * 100)\n",
    "         print(f\"▶ 4년 이상 근무자 수(이동 대상): {len(moved_4yrs_subset)}명, 이동률: {moved_percentage:.2f}%\")\n",
    "    else:\n",
    "        print(f\"▶ 4년 이상 근무자 수(이동 대상): 0명\")\n",
    "    print(f\"▶ 평균 거리 변화 (이전-현재): {diff_metric:.2f} (양수면 거리 감소)\")\n",
    "    break\n",
    "\n",
    "# 7. 최대 시도 횟수 내에 모든 조건을 만족하는 해를 찾지 못한 경우, 최종 실패 메시지 출력\n",
    "if not success:\n",
    "    print(f\"\\n⚠️ 최대 반복 횟수({max_attempts})에 도달했지만 모든 조건을 충족하는 결과를 찾지 못했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7857945-8954-4e80-9fb6-f61481e0bdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
