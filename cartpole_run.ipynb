{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*start test*\n",
      "episode:0, count:468, reward tot:468.0, reward avg:468.00\n",
      "episode:1, count:500, reward tot:500.0, reward avg:484.00\n",
      "episode:2, count:487, reward tot:487.0, reward avg:485.00\n",
      "episode:3, count:500, reward tot:500.0, reward avg:488.75\n",
      "episode:4, count:416, reward tot:416.0, reward avg:474.20\n",
      "episode:5, count:317, reward tot:317.0, reward avg:448.00\n",
      "episode:6, count:241, reward tot:241.0, reward avg:418.43\n",
      "episode:7, count:421, reward tot:421.0, reward avg:418.75\n",
      "episode:8, count:235, reward tot:235.0, reward avg:398.33\n",
      "episode:9, count:351, reward tot:351.0, reward avg:393.60\n",
      "*end test*\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 저장된 모델 로드 (.keras or .h5 형태로 저장된 Keras 모델 사용)\n",
    "# model = tf.keras.models.load_model('./model/dqn.keras')\n",
    "# model = tf.keras.models.load_model('./model/reinforce.keras')\n",
    "# model = tf.keras.models.load_model('./model/a2c_actor.keras')\n",
    "# model = tf.keras.models.load_model('./model/ppo_actor.keras')\n",
    "model = PPO.load('./model/ppo_cartpole_stable')\n",
    "\n",
    "# 환경 생성 시 render 모드 명시\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "input_size = env.observation_space.shape[0]\n",
    "\n",
    "reward_list = []\n",
    "print(\"*start test*\")\n",
    "for episode in range(10):\n",
    "    state, _ = env.reset()  # 최신 gym은 두 값을 반환\n",
    "    reward_tot = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        count += 1\n",
    "\n",
    "        state_t = np.reshape(state, [1, input_size]).astype(np.float32)\n",
    "\n",
    "        ## dqn, reinforce, a2c, ppo\n",
    "        # Q = model.predict(state_t, verbose=0)[0]\n",
    "        # action = np.argmax(Q)\n",
    "        \n",
    "        ## stable baselines\n",
    "        action, _ = model.predict(state)\n",
    "\n",
    "        state_next, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        reward_tot += reward\n",
    "        state = state_next\n",
    "\n",
    "    reward_list.append(reward_tot)\n",
    "    print(f\"episode:{episode}, count:{count}, reward tot:{reward_tot}, reward avg:{np.mean(reward_list):.2f}\")\n",
    "print(\"*end test*\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
