{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b9f5b7-89b7-4323-9262-618e59d6b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-03 07:07:36,416] A new study created in memory with name: no-name-1f7d903b-b95e-4ebd-8a90-26c17784d142\n",
      "[I 2025-04-03 07:08:44,994] Trial 0 finished with value: 218.05 and parameters: {'node_num': 112, 'actor_lr': 0.0038585321692404815, 'critic_lr': 0.00030119862008390195, 'epochs_cnt': 6, 'discount_rate': 0.9773412435247546, 'lambda_gae': 0.9696765285440372}. Best is trial 0 with value: 218.05.\n",
      "[I 2025-04-03 07:10:26,551] Trial 1 finished with value: 325.53 and parameters: {'node_num': 128, 'actor_lr': 0.0006987296340588978, 'critic_lr': 0.0001181861980175588, 'epochs_cnt': 9, 'discount_rate': 0.9943858638955163, 'lambda_gae': 0.9726290455601527}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:11:37,078] Trial 2 finished with value: 244.13 and parameters: {'node_num': 128, 'actor_lr': 0.006500115648153573, 'critic_lr': 0.0005152116218151408, 'epochs_cnt': 5, 'discount_rate': 0.9073762307627764, 'lambda_gae': 0.9844586157742938}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:12:57,866] Trial 3 finished with value: 227.825 and parameters: {'node_num': 16, 'actor_lr': 0.0013364214264005353, 'critic_lr': 1.1636355351112844e-05, 'epochs_cnt': 9, 'discount_rate': 0.9574374308513425, 'lambda_gae': 0.9616648691007367}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:13:21,569] Trial 4 finished with value: 21.36 and parameters: {'node_num': 96, 'actor_lr': 1.4044608907942307e-05, 'critic_lr': 0.0002622524486101414, 'epochs_cnt': 5, 'discount_rate': 0.9325522155170919, 'lambda_gae': 0.9195412981153755}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:13:38,018] Trial 5 finished with value: 38.515 and parameters: {'node_num': 80, 'actor_lr': 0.0006092886065841961, 'critic_lr': 0.008887006693693297, 'epochs_cnt': 2, 'discount_rate': 0.9460256355907197, 'lambda_gae': 0.961269000546837}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:14:53,991] Trial 6 finished with value: 195.545 and parameters: {'node_num': 80, 'actor_lr': 0.0004855369041424537, 'critic_lr': 1.098520821066283e-05, 'epochs_cnt': 9, 'discount_rate': 0.992892536712183, 'lambda_gae': 0.9153337882542639}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:15:25,517] Trial 7 finished with value: 25.335 and parameters: {'node_num': 16, 'actor_lr': 0.00020262164434573256, 'critic_lr': 0.00014752144172512229, 'epochs_cnt': 7, 'discount_rate': 0.98589959646464, 'lambda_gae': 0.9892600814243603}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:16:16,981] Trial 8 finished with value: 116.755 and parameters: {'node_num': 112, 'actor_lr': 0.00024471387629617454, 'critic_lr': 0.0001950812485828716, 'epochs_cnt': 7, 'discount_rate': 0.9791141020976493, 'lambda_gae': 0.9768675163564213}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:16:51,410] Trial 9 finished with value: 22.295 and parameters: {'node_num': 32, 'actor_lr': 1.5073133633536667e-05, 'critic_lr': 0.0006713002714868005, 'epochs_cnt': 8, 'discount_rate': 0.9880823110238934, 'lambda_gae': 0.9032112473163947}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:17:33,936] Trial 10 finished with value: 24.675 and parameters: {'node_num': 48, 'actor_lr': 6.162324502518579e-05, 'critic_lr': 4.093263539148408e-05, 'epochs_cnt': 10, 'discount_rate': 0.95992748155647, 'lambda_gae': 0.9379262642611174}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:18:39,922] Trial 11 finished with value: 237.905 and parameters: {'node_num': 128, 'actor_lr': 0.009136488782011567, 'critic_lr': 0.0016070095213913595, 'epochs_cnt': 4, 'discount_rate': 0.9025167483506488, 'lambda_gae': 0.9892974356084584}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:19:30,268] Trial 12 finished with value: 182.795 and parameters: {'node_num': 128, 'actor_lr': 0.00226430710678273, 'critic_lr': 0.0012833287328040868, 'epochs_cnt': 3, 'discount_rate': 0.9076590075752666, 'lambda_gae': 0.9435126674626299}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:19:39,367] Trial 13 finished with value: 20.255 and parameters: {'node_num': 112, 'actor_lr': 0.009389974876429664, 'critic_lr': 6.220778591673621e-05, 'epochs_cnt': 1, 'discount_rate': 0.9259731755027556, 'lambda_gae': 0.979449214302549}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:20:02,827] Trial 14 finished with value: 21.465 and parameters: {'node_num': 64, 'actor_lr': 8.55157015587058e-05, 'critic_lr': 0.00401063612204136, 'epochs_cnt': 5, 'discount_rate': 0.918154050116737, 'lambda_gae': 0.9550682169607184}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:21:09,213] Trial 15 finished with value: 206.5 and parameters: {'node_num': 128, 'actor_lr': 0.001216627752337615, 'critic_lr': 6.31831812317303e-05, 'epochs_cnt': 6, 'discount_rate': 0.970661141198927, 'lambda_gae': 0.9715123923951614}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:22:12,393] Trial 16 finished with value: 227.42 and parameters: {'node_num': 96, 'actor_lr': 0.0036723305117219327, 'critic_lr': 0.0006734465640028872, 'epochs_cnt': 4, 'discount_rate': 0.9375984893156437, 'lambda_gae': 0.9515581103328923}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:22:57,210] Trial 17 finished with value: 37.785 and parameters: {'node_num': 96, 'actor_lr': 9.499955881480155e-05, 'critic_lr': 0.00010769909591005381, 'epochs_cnt': 10, 'discount_rate': 0.9180408329379511, 'lambda_gae': 0.9296769808399379}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:24:03,748] Trial 18 finished with value: 191.73 and parameters: {'node_num': 64, 'actor_lr': 0.0008920635893827334, 'critic_lr': 2.478207037597942e-05, 'epochs_cnt': 7, 'discount_rate': 0.9641470353958209, 'lambda_gae': 0.981001969511802}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:25:18,216] Trial 19 finished with value: 209.98 and parameters: {'node_num': 128, 'actor_lr': 0.003930426438039848, 'critic_lr': 0.0008192062990129819, 'epochs_cnt': 8, 'discount_rate': 0.9978144798884381, 'lambda_gae': 0.9660049761923699}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:25:38,335] Trial 20 finished with value: 22.095 and parameters: {'node_num': 112, 'actor_lr': 3.993884421737218e-05, 'critic_lr': 0.0027021307793860375, 'epochs_cnt': 4, 'discount_rate': 0.9399794415937901, 'lambda_gae': 0.9831213775500915}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:26:07,281] Trial 21 finished with value: 61.01 and parameters: {'node_num': 128, 'actor_lr': 0.009001911380074218, 'critic_lr': 0.0021046929014261824, 'epochs_cnt': 4, 'discount_rate': 0.9001212194895452, 'lambda_gae': 0.9874781312637867}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:26:40,637] Trial 22 finished with value: 117.915 and parameters: {'node_num': 128, 'actor_lr': 0.006268757847394863, 'critic_lr': 0.0005407298746180649, 'epochs_cnt': 2, 'discount_rate': 0.9077137223346062, 'lambda_gae': 0.9748257106648687}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:27:17,368] Trial 23 finished with value: 116.07 and parameters: {'node_num': 112, 'actor_lr': 0.00189076626355056, 'critic_lr': 0.001592195393789193, 'epochs_cnt': 3, 'discount_rate': 0.902055105309226, 'lambda_gae': 0.9890777165121685}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:28:09,926] Trial 24 finished with value: 158.365 and parameters: {'node_num': 96, 'actor_lr': 0.00556278216876544, 'critic_lr': 0.0004061068639479033, 'epochs_cnt': 5, 'discount_rate': 0.919240050134729, 'lambda_gae': 0.9807066670681595}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:29:03,556] Trial 25 finished with value: 198.23 and parameters: {'node_num': 128, 'actor_lr': 0.0021151817453699553, 'critic_lr': 0.005161907001492813, 'epochs_cnt': 3, 'discount_rate': 0.911203907929936, 'lambda_gae': 0.9646393071400217}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:29:31,380] Trial 26 finished with value: 24.79 and parameters: {'node_num': 112, 'actor_lr': 0.0001596991168243558, 'critic_lr': 0.00010733027520109369, 'epochs_cnt': 6, 'discount_rate': 0.9289156147050001, 'lambda_gae': 0.9562822822456358}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:30:21,687] Trial 27 finished with value: 163.555 and parameters: {'node_num': 128, 'actor_lr': 0.002735294592303115, 'critic_lr': 0.001009187408059271, 'epochs_cnt': 4, 'discount_rate': 0.9526316858800823, 'lambda_gae': 0.9726893160466784}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:31:10,835] Trial 28 finished with value: 91.795 and parameters: {'node_num': 80, 'actor_lr': 0.0004202182499442707, 'critic_lr': 0.0003543543919009117, 'epochs_cnt': 8, 'discount_rate': 0.9140647398117537, 'lambda_gae': 0.9848302061540177}. Best is trial 1 with value: 325.53.\n",
      "[I 2025-04-03 07:32:38,727] Trial 29 finished with value: 328.445 and parameters: {'node_num': 96, 'actor_lr': 0.005372432964286116, 'critic_lr': 0.0002277168169569523, 'epochs_cnt': 5, 'discount_rate': 0.9745127000993128, 'lambda_gae': 0.970020616032837}. Best is trial 29 with value: 328.445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value:  -328.445\n",
      "  Params:  {'node_num': 96, 'actor_lr': 0.005372432964286116, 'critic_lr': 0.0002277168169569523, 'epochs_cnt': 5, 'discount_rate': 0.9745127000993128, 'lambda_gae': 0.970020616032837}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.initializers import GlorotNormal #(튜닝) 추가\n",
    "from tensorflow.keras.activations import elu\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "LOSS_CLIPPING = 0.2\n",
    "\n",
    "class Agent(object):\n",
    "    #(1) trial\n",
    "    def __init__(self, trial):\n",
    "\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "\n",
    "        #(2) 변수설정\n",
    "        self.node_num = trial.suggest_int(\"node_num\", 16, 128, step=16)\n",
    "        self.actor_lr = trial.suggest_float(\"actor_lr\", 1e-5, 1e-2, log=True)\n",
    "        self.critic_lr = trial.suggest_float(\"critic_lr\", 1e-5, 1e-2, log=True)\n",
    "        self.epochs_cnt = trial.suggest_int(\"epochs_cnt\", 1, 10)\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        self.discount_rate = trial.suggest_float(\"discount_rate\", 0.90, 0.999)\n",
    "        self.lambda_gae = trial.suggest_float(\"lambda_gae\", 0.90, 0.99)\n",
    "        self.penalty = -10\n",
    "\n",
    "        #(3) episode_num = 200\n",
    "        self.episode_num = 200\n",
    "        self.moving_avg_size = 20\n",
    "        self.reward_list = []\n",
    "        self.count_list = []\n",
    "        self.moving_avg_list = []\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "        self.old_probs = []\n",
    "\n",
    "    def build_actor(self):\n",
    "        input_states = Input(shape=(self.state_size,), name='input_states')\n",
    "\n",
    "        x = Dense(self.node_num, activation='swish', kernel_initializer=GlorotNormal())(input_states)  \n",
    "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_states, outputs=out_actions)\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=self.actor_lr, clipnorm=1.0))\n",
    "        return model\n",
    "\n",
    "    def build_critic(self):\n",
    "        input_states = Input(shape=(self.state_size,), name='input_states')\n",
    "\n",
    "        x = Dense(self.node_num, activation='swish', kernel_initializer=GlorotNormal())(input_states)  \n",
    "        out_value = Dense(1, activation='linear', name='value')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_states, outputs=out_value)\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=self.critic_lr, clipnorm=1.0), loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episode_num):\n",
    "\n",
    "            state, _ = self.env.reset()\n",
    "            reward_tot = 0\n",
    "            step_count = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action, prob = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if done and step_count < 499:\n",
    "                    reward = self.penalty\n",
    "\n",
    "                self.store_transition(state, action, reward, done, next_state, prob)\n",
    "                state = next_state\n",
    "                reward_tot += reward\n",
    "                step_count += 1\n",
    "\n",
    "            self.reward_list.append(reward_tot - self.penalty)\n",
    "            self.count_list.append(step_count)\n",
    "            self.moving_avg_list.append(np.mean(self.reward_list[-self.moving_avg_size:]))\n",
    "\n",
    "            self.update_models()\n",
    "            self.clear_memory()\n",
    "\n",
    "        #(4) 전체 성과 반환 (목적 함수용)\n",
    "        return np.mean(self.reward_list)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state_input = np.reshape(state, [1, self.state_size]).astype(np.float32)\n",
    "\n",
    "        prob = self.actor(state_input, training=False).numpy()[0]\n",
    "\n",
    "        action = np.random.choice(self.action_size, p=prob)\n",
    "\n",
    "        return action, prob\n",
    "\n",
    "    def store_transition(self, state, action, reward, done, next_state, prob):\n",
    "        action_onehot = np.zeros(self.action_size)\n",
    "        action_onehot[action] = 1.0\n",
    "\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.next_states.append(next_state)\n",
    "        self.old_probs.append(prob)\n",
    "\n",
    "    def update_models(self):\n",
    "        states = np.array(self.states, dtype=np.float32)\n",
    "        next_states = np.array(self.next_states, dtype=np.float32)\n",
    "        rewards = np.array(self.rewards, dtype=np.float32).reshape(-1, 1)\n",
    "        dones = np.array(self.dones, dtype=np.int32).reshape(-1, 1)\n",
    "        actions = np.array(self.actions, dtype=np.float32)\n",
    "        old_probs = np.array(self.old_probs, dtype=np.float32)\n",
    "\n",
    "        advantages, targets = self.compute_gae(states, next_states, rewards, dones)\n",
    "        advantages -= np.mean(advantages)\n",
    "        advantages /= (np.std(advantages) + 1e-8)\n",
    "\n",
    "        advantages = advantages.astype(np.float32)\n",
    "        targets = targets.astype(np.float32)\n",
    "\n",
    "        for _ in range(self.epochs_cnt):\n",
    "            with tf.GradientTape() as tape:\n",
    "                probs = self.actor(states, training=True)\n",
    "                new_probs = tf.reduce_sum(actions * probs, axis=1, keepdims=True)\n",
    "                old_probs_sum = tf.reduce_sum(actions * old_probs, axis=1, keepdims=True)\n",
    "\n",
    "                ratio = new_probs / (old_probs_sum + 1e-10)\n",
    "                clipped_ratio = tf.clip_by_value(ratio, 1 - LOSS_CLIPPING, 1 + LOSS_CLIPPING)\n",
    "                actor_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))\n",
    "\n",
    "            gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            self.actor.optimizer.apply_gradients(zip(gradients, self.actor.trainable_variables))\n",
    "\n",
    "            self.critic.train_on_batch(states, targets)\n",
    "\n",
    "    def compute_gae(self, states, next_states, rewards, dones):\n",
    "\n",
    "        values = self.critic(states, training=False).numpy()\n",
    "        next_values = self.critic(next_states, training=False).numpy()\n",
    "\n",
    "        advantages = np.zeros_like(rewards, dtype=np.float32)\n",
    "        targets = np.zeros_like(rewards, dtype=np.float32)\n",
    "\n",
    "        gae = 0.0\n",
    "\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.discount_rate * next_values[t] * (1 - dones[t]) - values[t]\n",
    "\n",
    "            gae = delta + self.discount_rate * self.lambda_gae * (1 - dones[t]) * gae\n",
    "\n",
    "            advantages[t] = gae\n",
    "            targets[t] = gae + values[t]\n",
    "        return advantages, targets\n",
    "\t\t\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "        self.old_probs = []\n",
    "\n",
    "#(5) Optuna 목적 함수\n",
    "def objective(trial):\n",
    "    agent = Agent(trial)\n",
    "    avg_reward = agent.train()\n",
    "    return avg_reward\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    #(6) Optuna 최적화 실행\n",
    "    study = optuna.create_study(direction=\"maximize\")  # reward 최대화\n",
    "    study.optimize(objective, n_trials=30)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    print(\"  Value: \", study.best_value)\n",
    "    print(\"  Params: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a313e47-d3ab-4f88-8d44-b8f568b75152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
